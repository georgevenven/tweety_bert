{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Data Into Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "mins_per_fold = 50\n",
    "fold_data_dir = \"/media/george-vengrovski/disk1/decoder_data\"\n",
    "\n",
    "birds_wav_paths = [\n",
    "    \"/media/george-vengrovski/disk2/canary/yarden_data/llb3_data/llb3_songs\",\n",
    "    \"/media/george-vengrovski/disk2/canary/yarden_data/llb11_data/llb11_songs\",\n",
    "    \"/media/george-vengrovski/disk2/canary/yarden_data/llb16_data/llb16_songs\"\n",
    "]\n",
    "\n",
    "song_detection_json = \"files/contains_llb.json\"\n",
    "\n",
    "# Build a mapping from filename to its full path for all birds\n",
    "wav_file_to_path = {}\n",
    "for bird_path in birds_wav_paths:\n",
    "    if os.path.isdir(bird_path):\n",
    "        for fname in os.listdir(bird_path):\n",
    "            if fname.endswith('.wav'):\n",
    "                wav_file_to_path[fname] = os.path.join(bird_path, fname)\n",
    "\n",
    "# Parse the song detection JSON and collect song files and their durations per bird\n",
    "with open(song_detection_json, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "bird_song_files = {}  # bird_id -> list of (filename, duration_seconds)\n",
    "for entry in data:\n",
    "    if not entry.get('song_present', False):\n",
    "        continue\n",
    "    filename = entry['filename']\n",
    "    if filename not in wav_file_to_path:\n",
    "        continue\n",
    "    bird_id = filename.split('_')[0]\n",
    "    total_duration = 0.0\n",
    "    for seg in entry.get('segments', []):\n",
    "        onset_ms = seg.get('onset_ms', 0)\n",
    "        offset_ms = seg.get('offset_ms', 0)\n",
    "        total_duration += (offset_ms - onset_ms) / 1000.0\n",
    "    if total_duration <= 0:\n",
    "        continue\n",
    "    if bird_id not in bird_song_files:\n",
    "        bird_song_files[bird_id] = []\n",
    "    bird_song_files[bird_id].append((filename, total_duration))\n",
    "\n",
    "# For each bird, randomly assign files to folds so each fold has at least mins_per_fold minutes\n",
    "folds_info = {}  # bird_id -> list of folds, each fold is list of (filename, duration)\n",
    "for bird_id, files in bird_song_files.items():\n",
    "    random.shuffle(files)\n",
    "    folds = []\n",
    "    current_fold = []\n",
    "    current_fold_duration = 0.0\n",
    "    for fname, dur in files:\n",
    "        current_fold.append((fname, dur))\n",
    "        current_fold_duration += dur\n",
    "        if current_fold_duration >= mins_per_fold * 60:\n",
    "            folds.append(current_fold)\n",
    "            current_fold = []\n",
    "            current_fold_duration = 0.0\n",
    "    if current_fold:  # Add any remaining files to a final fold\n",
    "        folds.append(current_fold)\n",
    "    folds_info[bird_id] = folds\n",
    "\n",
    "# Copy files to their respective fold directories with progress bar\n",
    "for bird_id, folds in folds_info.items():\n",
    "    for i, fold in enumerate(folds):\n",
    "        fold_dir = os.path.join(fold_data_dir, bird_id, f\"fold{i+1}\")\n",
    "        os.makedirs(fold_dir, exist_ok=True)\n",
    "        print(f\"Copying files for {bird_id} fold {i+1} ({len(fold)} files)...\")\n",
    "        for fname, _ in tqdm(fold, desc=f\"{bird_id} fold{i+1}\", leave=False):\n",
    "            src = wav_file_to_path[fname]\n",
    "            dst = os.path.join(fold_dir, fname)\n",
    "            if not os.path.exists(dst):\n",
    "                shutil.copy2(src, dst)\n",
    "\n",
    "# Gather fold durations for plotting\n",
    "plot_bird_ids = []\n",
    "plot_fold_names = []\n",
    "plot_fold_minutes = []\n",
    "for bird_id, folds in folds_info.items():\n",
    "    for i, fold in enumerate(folds):\n",
    "        fold_minutes = sum(dur for _, dur in fold) / 60\n",
    "        plot_bird_ids.append(bird_id)\n",
    "        plot_fold_names.append(f\"fold{i+1}\")\n",
    "        plot_fold_minutes.append(fold_minutes)\n",
    "\n",
    "# Plot bar plots showing minutes per fold for each bird\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "bar_labels = [f\"{bird}-{fold}\" for bird, fold in zip(plot_bird_ids, plot_fold_names)]\n",
    "bars = plt.bar(bar_labels, plot_fold_minutes, color='skyblue')\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.1f}',\n",
    "             ha='center', va='bottom')\n",
    "plt.title('Minutes of Song Data per Fold (per Bird)', fontsize=14, pad=20)\n",
    "plt.xlabel('Bird-Fold', fontsize=12)\n",
    "plt.ylabel('Total Duration (minutes)', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We want to create embeddings for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "\n",
    "import os\n",
    "os.environ['MKL_THREADING_LAYER'] = 'GNU'  # Add this before running your code\n",
    "\n",
    "decoding_module = importlib.import_module(\"decoding\")\n",
    "\n",
    "class Args:\n",
    "    def __init__(self, mode, bird_name, model_name, wav_folder, song_detection_json_path, num_samples_umap):\n",
    "        self.mode = mode\n",
    "        self.bird_name = bird_name\n",
    "        self.model_name = model_name\n",
    "        self.wav_folder = wav_folder\n",
    "        self.song_detection_json_path = song_detection_json_path\n",
    "        self.num_samples_umap = num_samples_umap\n",
    "        self.num_random_files_spec = 1000  # Default value\n",
    "        self.single_threaded_spec = False  # Default value\n",
    "        self.nfft = 1024  # Default value\n",
    "        self.raw_spectrogram_umap = False  # Default value for store_true flag\n",
    "        self.state_finding_algorithm_umap = \"HDBSCAN\"  # Default value\n",
    "        self.context_umap = 1000  # Default value\n",
    "\n",
    "for root, dirs, files in os.walk(fold_data_dir):\n",
    "    for dir in dirs:\n",
    "        if \"fold\" in dir:\n",
    "            bird = os.path.basename(root)\n",
    "            bird_name_fold = f\"{bird}_{dir}\"\n",
    "            wav_folder = os.path.join(root, dir)\n",
    "            args = Args(\n",
    "                mode=\"single\",\n",
    "                bird_name=bird_name_fold,\n",
    "                model_name=\"BF_Canary_Joint_Run\",\n",
    "                wav_folder=wav_folder,\n",
    "                song_detection_json_path=song_detection_json,\n",
    "                num_samples_umap=\"1e6\"\n",
    "            )\n",
    "            print(f\"Running decoding.py --mode single --bird_name {bird_name_fold} --model_name BF_Canary_Joint_Run --wav_folder {wav_folder} --song_detection_json_path {song_detection_json} --num_samples_umap 1e6\")\n",
    "            decoding_module.main(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalulate TweetyBERT, Parameteric, Load and Transform on Folds FREEEEEEEEEEEEZE IT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, pathlib, shutil, time, gc\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "# ── we only need classic UMAP for logits viz ───────────────────────────────\n",
    "import umap\n",
    "# from umap.parametric_umap import ParametricUMAP\n",
    "# import hdbscan\n",
    "# from hdbscan import approximate_predict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ── constants ────────────────────────────────────────────────────────────────\n",
    "ROOT       = pathlib.Path().resolve()\n",
    "NPZ_DIR    = ROOT / \"files\"\n",
    "SAVE_DIR   = ROOT / \"results\" / \"decoder_eval\"\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CSV_PATH   = SAVE_DIR / \"timings.csv\"      # central log for resumption\n",
    "\n",
    "RAW_FILES = [\n",
    "    # \"llb3_fold1.npz\",\n",
    "    # \"llb3_fold2.npz\",\n",
    "    # \"llb3_fold3.npz\",\n",
    "    # \"llb3_fold4.npz\",\n",
    "    # \"llb3_fold5.npz\",\n",
    "    # \"llb3_fold6.npz\",\n",
    "    # \"llb3_fold7.npz\",\n",
    "    # \"llb3_fold8.npz\",\n",
    "    # \"llb3_fold9.npz\",\n",
    "    # \"llb11_fold1.npz\",\n",
    "    # \"llb11_fold2.npz\",\n",
    "    # \"llb11_fold3.npz\",\n",
    "    # \"llb11_fold4.npz\",\n",
    "    # \"llb11_fold5.npz\",\n",
    "    # \"llb11_fold6.npz\",\n",
    "    # \"llb11_fold7.npz\",\n",
    "    # \"llb11_fold8.npz\",\n",
    "    # \"llb16_fold1.npz\",\n",
    "    # \"llb16_fold2.npz\",\n",
    "    # \"llb16_fold3.npz\",\n",
    "    \"llb16_fold4.npz\",\n",
    "    \"llb16_fold5.npz\"\n",
    "]\n",
    "\n",
    "MAX_FRAMES = 1_000_000\n",
    "CTX        = 1_000\n",
    "DEV        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_DIR  = ROOT / \"experiments\" / \"BF_Canary_Joint_Run\"\n",
    "\n",
    "# ── helpers: i/o + tiny utils ────────────────────────────────────────────────\n",
    "def group_by_bird(fnames: List[str]) -> Dict[str, List[str]]:\n",
    "    d = defaultdict(list)\n",
    "    for f in fnames:\n",
    "        d[f.split(\"_\")[0]].append(f)\n",
    "    return {k: sorted(v) for k, v in d.items()}\n",
    "\n",
    "def load_npz(fp: pathlib.Path) -> Tuple[np.ndarray, ...]:\n",
    "    with np.load(fp) as f:\n",
    "        return (\n",
    "            f[\"predictions\"][:MAX_FRAMES],      # embeddings\n",
    "            f[\"s\"][:MAX_FRAMES],                # spectrograms\n",
    "            f[\"hdbscan_labels\"][:MAX_FRAMES],   # labels from training fold\n",
    "            f[\"ground_truth_labels\"][:MAX_FRAMES],\n",
    "        )\n",
    "\n",
    "# ── helpers: decoder forward ────────────────────────────────────────────────\n",
    "sys.path.insert(0, str(pathlib.Path(\"src\").resolve()))\n",
    "from src.decoder import TweetyBertClassifier, SongDataSet_Image, CollateFunction\n",
    "def run_decoder(model_torch: torch.nn.Module,\n",
    "                specs: np.ndarray,\n",
    "                gt: np.ndarray,\n",
    "                tmp_dir: pathlib.Path) -> Tuple[np.ndarray, float, np.ndarray]:\n",
    "    tmp_dir.mkdir(parents=True, exist_ok=True)\n",
    "    seg_id = 0\n",
    "    for start in range(0, len(specs), CTX):\n",
    "        seg = specs[start:start + CTX]\n",
    "        seg = np.pad(seg, ((0, 0), (20, 0)), constant_values=0)\n",
    "        if seg.shape[0] < CTX:\n",
    "            seg = np.pad(seg, ((0, CTX - seg.shape[0]), (0, 0)))\n",
    "        gt_seg = gt[start:start + CTX]\n",
    "        if gt_seg.shape[0] < CTX:\n",
    "            gt_seg = np.pad(gt_seg, (0, CTX - gt_seg.shape[0]))\n",
    "        # write the slice so the dataset isn't empty\n",
    "        np.savez(\n",
    "            tmp_dir / f\"{seg_id}.npz\",\n",
    "            labels=gt_seg,\n",
    "            s=seg.T,                        # freq × time, per decoder expectations\n",
    "            vocalization=np.zeros(CTX, dtype=np.int8)\n",
    "        )\n",
    "        seg_id += 1\n",
    "\n",
    "    if seg_id == 0:\n",
    "        raise RuntimeError(\"no segments saved – tmp_dir empty\")\n",
    "\n",
    "    ds = SongDataSet_Image(tmp_dir,\n",
    "                           num_classes=int(gt.max()) + 1,\n",
    "                           segment_length=CTX,\n",
    "                           infinite_loader=False)\n",
    "    dl = DataLoader(ds, batch_size=1, shuffle=False,\n",
    "                    collate_fn=CollateFunction(segment_length=CTX))\n",
    "\n",
    "    preds, logits_accum, gts_accum = [], [], []\n",
    "    t0 = time.perf_counter()\n",
    "    model_torch.eval()\n",
    "    with torch.no_grad():\n",
    "        for b in dl:\n",
    "            s = b[0].to(DEV)           # first element = spectrogram tensor\n",
    "            gt = b[1].to(DEV)\n",
    "            logits = model_torch(s)          # (1, S, C)\n",
    "            preds.append(torch.argmax(logits, 2).cpu().numpy())\n",
    "            logits_accum.append(logits.cpu().numpy())        # (1, S, C)\n",
    "            gts_accum.append(torch.argmax(gt, 2).cpu().numpy())\n",
    "\n",
    "    t_elapsed = time.perf_counter() - t0\n",
    "\n",
    "    shutil.rmtree(tmp_dir, ignore_errors=True)\n",
    "    preds   = np.concatenate([p.squeeze(0) for p in preds])[:len(specs)]\n",
    "    logits  = np.concatenate([l.squeeze(0) for l in logits_accum])[:len(specs)]\n",
    "    gts     = np.concatenate([g.squeeze(0) for g in gts_accum])[:len(specs)]\n",
    "\n",
    "    return preds, t_elapsed, logits, gts\n",
    "\n",
    "# ── per-bird benchmark ───────────────────────────────────────────────────────\n",
    "def benchmark_bird(bird_id: str, fold_files: List[str]) -> None:\n",
    "    fold_paths = [NPZ_DIR / f for f in fold_files]\n",
    "    csv_rows   = []\n",
    "\n",
    "    # ── resume support ────────────────────────────────────────────────────\n",
    "    # read anything we've already logged so we can skip it on rerun\n",
    "    if CSV_PATH.exists():\n",
    "        _done_df = pd.read_csv(CSV_PATH)\n",
    "        done_keys = {\n",
    "            (row.bird, row.fit_fold, row.eval_fold)\n",
    "            for _, row in _done_df.iterrows()\n",
    "        }\n",
    "    else:\n",
    "        done_keys = set()\n",
    "\n",
    "    for fit_path in fold_paths:\n",
    "        fit_name = fit_path.name\n",
    "\n",
    "        # if every possible (fit, eval) pair is done already, skip this fold\n",
    "        remaining_eval_paths = [\n",
    "            p for p in fold_paths\n",
    "            if p != fit_path and (bird_id, fit_name, p.name) not in done_keys\n",
    "        ]\n",
    "        if not remaining_eval_paths:\n",
    "            print(f\"[resume] {bird_id} · {fit_name}: all evals finished – skipping\")\n",
    "            continue\n",
    "\n",
    "        # --- Compute class weights from hdbscan_labels in fit fold ---\n",
    "        with np.load(fit_path) as f:\n",
    "            hdbscan_labels = f[\"hdbscan_labels\"][:MAX_FRAMES]\n",
    "        # Only use non-negative labels (ignore noise if present)\n",
    "        valid_labels = hdbscan_labels[hdbscan_labels >= 0]\n",
    "        label_counts = Counter(valid_labels)\n",
    "        num_classes = int(valid_labels.max()) + 1 if valid_labels.size > 0 else 1\n",
    "        class_counts = np.array([label_counts.get(i, 0) for i in range(num_classes)])\n",
    "        # Avoid division by zero\n",
    "        class_counts[class_counts == 0] = 1\n",
    "        class_weights = 1.0 / class_counts\n",
    "        class_weights = class_weights / class_weights.sum() * num_classes\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float32, device=DEV)\n",
    "\n",
    "        # train linear probe\n",
    "        dec_dir = SAVE_DIR / f\"{bird_id}__{fit_name}__decoder\"\n",
    "        probe   = TweetyBertClassifier(model_dir=str(MODEL_DIR),\n",
    "                                       linear_decoder_dir=str(dec_dir),\n",
    "                                       context_length=CTX,\n",
    "                                       weight=class_weights)\n",
    "        probe.prepare_data(str(fit_path), test_train_split=0.8)\n",
    "        probe.create_dataloaders(batch_size=42)\n",
    "        probe.create_classifier()\n",
    "        probe.train_classifier(lr=1e-3,\n",
    "                               desired_total_batches=450,\n",
    "                               batches_per_eval=25,\n",
    "                               patience=4)\n",
    "        \n",
    "        probe_model = probe.classifier_model.to(DEV)\n",
    "\n",
    "        for eval_path in remaining_eval_paths:\n",
    "            k = f\"{fit_name}__{eval_path.name}\"\n",
    "            res_path = SAVE_DIR / f\"{k}.results.npz\"\n",
    "            if res_path.exists():\n",
    "                continue\n",
    "\n",
    "            emb_eval, specs_eval, _, gt_eval = load_npz(eval_path)\n",
    "\n",
    "            # --- decoder forward pass ---\n",
    "            lbl_d, t_d, logits_eval, gt_eval = run_decoder(\n",
    "                probe_model, specs_eval, gt_eval, SAVE_DIR / \"tmp_eval\"\n",
    "            )\n",
    "\n",
    "            # --- UMAP on logits for visualization ---\n",
    "            n_umap = min(500_000, logits_eval.shape[0])\n",
    "            if n_umap > 1:\n",
    "                um = umap.UMAP(\n",
    "                    n_neighbors=30,\n",
    "                    min_dist=0.0,\n",
    "                    n_components=2,\n",
    "                    metric=\"cosine\",\n",
    "                    n_jobs=-1,\n",
    "                    low_memory=True\n",
    "                )\n",
    "                Z = um.fit_transform(logits_eval[:n_umap])\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                plt.scatter(\n",
    "                    Z[:, 0], Z[:, 1],\n",
    "                    c=gt_eval[:n_umap],\n",
    "                    cmap=\"tab20\",\n",
    "                    s=5,\n",
    "                    alpha=0.6\n",
    "                )\n",
    "                plt.title(f\"UMAP of decoder logits – {bird_id} · {k}\")\n",
    "                plt.xlabel(\"UMAP-1\")\n",
    "                plt.ylabel(\"UMAP-2\")\n",
    "                plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(SAVE_DIR / f\"{k}_logits_umap.png\", dpi=150)\n",
    "                plt.close()\n",
    "\n",
    "            # --- UMAP / PUMAP labels remain disabled ---\n",
    "            lbl_u = np.zeros(len(emb_eval), dtype=int)\n",
    "            lbl_p = np.zeros(len(emb_eval), dtype=int)\n",
    "            t_u   = t_p = 0.0\n",
    "\n",
    "            np.savez_compressed(res_path,\n",
    "                                umap_labels=lbl_u,\n",
    "                                pumap_labels=lbl_p,\n",
    "                                decoder_labels=lbl_d,\n",
    "                                ground_truth_labels=gt_eval)\n",
    "\n",
    "            n_rows_emb = len(emb_eval)\n",
    "            n_rows_dec = len(specs_eval)\n",
    "            csv_rows.append(dict(bird=bird_id,\n",
    "                                 fit_fold=fit_name,\n",
    "                                 eval_fold=eval_path.name,\n",
    "                                 umap_s_per_row=0,\n",
    "                                 pumap_s_per_row=0,\n",
    "                                 decoder_s_per_row=t_d / n_rows_dec,\n",
    "                                 results_path=str(res_path)))\n",
    "\n",
    "        shutil.rmtree(dec_dir, ignore_errors=True)\n",
    "        # release gpu tensors\n",
    "        probe_model.to('cpu')\n",
    "        probe.classifier_model.to('cpu')\n",
    "        # drop optimizer buffers if present\n",
    "        if hasattr(probe, 'optimizer'):\n",
    "            del probe.optimizer\n",
    "        # clear references\n",
    "        del probe_model\n",
    "        del probe\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # only append if we actually added new work this run\n",
    "    if csv_rows:\n",
    "        pd.DataFrame(csv_rows).to_csv(\n",
    "            CSV_PATH,\n",
    "            mode=\"a\",\n",
    "            header=not CSV_PATH.exists(),\n",
    "            index=False\n",
    "        )\n",
    "\n",
    "# ── driver ───────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    for bird, files in group_by_bird(RAW_FILES).items():\n",
    "        benchmark_bird(bird, files)\n",
    "\n",
    "    df = pd.read_csv(SAVE_DIR / \"timings.csv\")\n",
    "    print(\"\\n--- mean sec / frame ---\")\n",
    "    print(df.groupby(\"bird\")[[\"umap_s_per_row\",\n",
    "                              \"pumap_s_per_row\",\n",
    "                              \"decoder_s_per_row\"]].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics import v_measure_score\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "from collections import Counter\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# ClusteringMetrics Class (calculates metrics and generates dashboard plot)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "class ClusteringMetrics:\n",
    "    \"\"\"Evaluate clustering vs. ground‑truth phrase labels.\"\"\"\n",
    "\n",
    "    def __init__(self, gt: np.ndarray, pred: np.ndarray, silence: int = 0):\n",
    "        if gt.shape != pred.shape:\n",
    "            # Try to truncate the longer array if lengths differ by a small margin\n",
    "            # This can happen if pred_dec was based on spec length and gt was slightly different\n",
    "            min_len = min(len(gt), len(pred))\n",
    "            if abs(len(gt) - len(pred)) > 100: # Arbitrary threshold for \"small margin\"\n",
    "                 raise ValueError(f\"gt (shape {gt.shape}) and pred (shape {pred.shape}) arrays must have identical shape or be very close.\")\n",
    "            print(f\"Warning: GT and Pred shapes differ ({gt.shape} vs {pred.shape}). Truncating to shortest length: {min_len}\")\n",
    "            gt = gt[:min_len]\n",
    "            pred = pred[:min_len]\n",
    "\n",
    "        self.gt_raw = gt.astype(int)\n",
    "        self.pred = pred.astype(int)\n",
    "        self.gt = self._merge_silence(self.gt_raw, silence) # Processed GT\n",
    "\n",
    "        self.gt_types = np.unique(self.gt)\n",
    "        self.pred_types = np.unique(self.pred)\n",
    "\n",
    "        self._build_confusion()\n",
    "        self.mapping = self._hungarian()\n",
    "\n",
    "    @staticmethod\n",
    "    def _merge_silence(arr: np.ndarray, silence: int) -> np.ndarray:\n",
    "        \"\"\"Fill contiguous *silence* runs with the nearest neighbour label.\"\"\"\n",
    "        if arr.size == 0:\n",
    "            return arr\n",
    "        out = arr.copy()\n",
    "        i = 0\n",
    "        while i < len(out):\n",
    "            if out[i] != silence:\n",
    "                i += 1\n",
    "                continue\n",
    "            j = i\n",
    "            while j < len(out) and out[j] == silence:\n",
    "                j += 1\n",
    "            \n",
    "            # Determine fill value\n",
    "            # Prefer left non-silence, then right, then keep silence if surrounded\n",
    "            left_val = out[i-1] if i > 0 and out[i-1] != silence else None\n",
    "            right_val = out[j] if j < len(out) and out[j] != silence else None\n",
    "\n",
    "            if left_val is not None:\n",
    "                fill = left_val\n",
    "            elif right_val is not None:\n",
    "                fill = right_val\n",
    "            else: # Both neighbors are silence or out of bounds\n",
    "                # If it's an isolated block of silence or all silence, keep as silence\n",
    "                # Or, if you have a default non-silence label, use that.\n",
    "                # For now, if no non-silence neighbor, it will effectively extend the last seen non-silence or first seen.\n",
    "                # A better strategy might be needed if all is silence or starts/ends with long silence.\n",
    "                # Using a simpler logic: if left exists, use it, else if right exists, use it, else keep silence.\n",
    "                left_neighbor = out[i - 1] if i > 0 else None\n",
    "                right_neighbor = out[j] if j < len(out) else None\n",
    "                \n",
    "                if left_neighbor is not None and left_neighbor != silence:\n",
    "                    fill = left_neighbor\n",
    "                elif right_neighbor is not None and right_neighbor != silence:\n",
    "                    fill = right_neighbor\n",
    "                else: # if surrounded by silence or at edges with silence\n",
    "                    # find first non-silence from start if available\n",
    "                    first_nonsilence_overall = next((val for val in arr if val != silence), silence)\n",
    "                    fill = first_nonsilence_overall\n",
    "\n",
    "            out[i:j] = fill\n",
    "            i = j\n",
    "        return out\n",
    "\n",
    "    def _build_confusion(self) -> None:\n",
    "        \"\"\"GT×Pred frame counts + column‑normalised version.\"\"\"\n",
    "        if not self.gt_types.size or not self.pred_types.size:\n",
    "            self.C = np.array([], dtype=int)\n",
    "            self.C_norm = np.array([], dtype=float)\n",
    "            return\n",
    "\n",
    "        gt_idx = {l: i for i, l in enumerate(self.gt_types)}\n",
    "        pr_idx = {l: i for i, l in enumerate(self.pred_types)}\n",
    "        self.C = np.zeros((len(self.gt_types), len(self.pred_types)), dtype=int)\n",
    "        \n",
    "        for g_val, p_val in zip(self.gt, self.pred):\n",
    "            if g_val in gt_idx and p_val in pr_idx: # Ensure labels are in unique sets\n",
    "                 np.add.at(self.C, (gt_idx[g_val], pr_idx[p_val]), 1)\n",
    "        \n",
    "        col_sum = self.C.sum(axis=0, keepdims=True)\n",
    "        with np.errstate(divide='ignore', invalid='ignore'): # Handle division by zero for columns with no predictions\n",
    "            self.C_norm = np.divide(self.C, col_sum, where=col_sum != 0, out=np.zeros_like(self.C, dtype=float))\n",
    "\n",
    "\n",
    "    def _hungarian(self) -> Dict[int, int]:\n",
    "        if not hasattr(self, 'C_norm') or self.C_norm.size == 0:\n",
    "            return {}\n",
    "        # Cost matrix for Hungarian algorithm should maximize overlap, so use negative C_norm\n",
    "        cost_matrix = -self.C_norm\n",
    "        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "        \n",
    "        mapping = {}\n",
    "        # Ensure indices are within bounds\n",
    "        for r, c in zip(row_ind, col_ind):\n",
    "            if r < len(self.gt_types) and c < len(self.pred_types):\n",
    "                mapping[self.gt_types[r]] = self.pred_types[c]\n",
    "        return mapping\n",
    "\n",
    "    def v_measure(self) -> float:\n",
    "        if self.gt.size == 0 or self.pred.size == 0:\n",
    "            return 0.0\n",
    "        # V-measure can't handle if one of the arrays is all one label and the other is different\n",
    "        # Or if arrays are empty after potential initial processing.\n",
    "        try:\n",
    "            return v_measure_score(self.gt, self.pred)\n",
    "        except ValueError:\n",
    "             # This can happen if, for example, gt or pred contains only one unique label\n",
    "             # and it doesn't align in a way v_measure_score expects, or if labels are negative.\n",
    "             # A simple check: if number of unique labels is 1 for both and they are same, V is 1, else 0.\n",
    "            if len(self.gt_types) == 1 and len(self.pred_types) == 1:\n",
    "                return 1.0 if self.gt_types[0] == self.pred_types[0] else 0.0\n",
    "            return 0.0 # Default for other ValueErrors\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # three flavours of frame-error-rate\n",
    "    # ------------------------------------------------------------------\n",
    "    def _fer_generic(\n",
    "        self,\n",
    "        use_mask: Optional[np.ndarray] = None\n",
    "    ) -> float:\n",
    "        \"\"\"helper: compute fer over the whole array or a masked subset.\"\"\"\n",
    "        if self.gt.size == 0:\n",
    "            return 0.0\n",
    "\n",
    "        if use_mask is None:\n",
    "            use_mask = np.ones_like(self.gt, dtype=bool)\n",
    "\n",
    "        masked_gt   = self.gt[use_mask]\n",
    "        masked_pred = self.pred[use_mask]\n",
    "\n",
    "        if masked_gt.size == 0:\n",
    "            return 0.0\n",
    "\n",
    "        correct = 0\n",
    "        for g, p in zip(masked_gt, masked_pred):\n",
    "            if g in self.mapping and self.mapping[g] == p:\n",
    "                correct += 1\n",
    "        return 100.0 * (1.0 - correct / masked_gt.size)\n",
    "\n",
    "    def total_fer(self) -> float:\n",
    "        \"\"\"error rate across *all* frames (current behaviour).\"\"\"\n",
    "        return self._fer_generic()\n",
    "\n",
    "    def matched_fer(self) -> float:\n",
    "        \"\"\"error rate restricted to frames whose GT label is mapped.\"\"\"\n",
    "        mapped_mask = np.isin(self.gt, list(self.mapping.keys()))\n",
    "        return self._fer_generic(use_mask=mapped_mask)\n",
    "\n",
    "    # keep old name as alias for backwards compat\n",
    "    def frame_error_rate(self) -> float:\n",
    "        return self.total_fer()\n",
    "\n",
    "    def macro_fer(self) -> float:\n",
    "        if not self.gt_types.size:\n",
    "            return 0.0\n",
    "        \n",
    "        per_type_fer = []\n",
    "        for gt_label_type in self.gt_types:\n",
    "            # Frames corresponding to this ground truth type\n",
    "            type_mask = (self.gt == gt_label_type)\n",
    "            if not np.any(type_mask): # No frames for this GT type\n",
    "                continue\n",
    "\n",
    "            # If this GT type is not in the mapping, all its frames are errors for Macro FER\n",
    "            if gt_label_type not in self.mapping:\n",
    "                per_type_fer.append(1.0) # 100% error for this unmapped type\n",
    "                continue\n",
    "\n",
    "            mapped_pred_label = self.mapping[gt_label_type]\n",
    "            \n",
    "            # Calculate errors for this type\n",
    "            errors_for_type = np.sum(self.pred[type_mask] != mapped_pred_label)\n",
    "            total_for_type = np.sum(type_mask)\n",
    "            \n",
    "            per_type_fer.append(errors_for_type / total_for_type if total_for_type > 0 else 0.0)\n",
    "            \n",
    "        return 100.0 * np.mean(per_type_fer) if per_type_fer else 0.0\n",
    "\n",
    "    def stats(self) -> Dict[str, Any]:\n",
    "        if self.gt.size == 0: # Handle empty case\n",
    "            return dict(\n",
    "                pct_types_mapped=0, pct_frames_mapped=0,\n",
    "                mapped_counts={}, unmapped_counts={},\n",
    "            )\n",
    "        counts = {g: (self.gt == g).sum() for g in self.gt_types}\n",
    "        mapped_gt_types = set(self.mapping.keys())\n",
    "        \n",
    "        mapped_frames = 0\n",
    "        for gt_label in mapped_gt_types:\n",
    "            if gt_label in counts:\n",
    "                mapped_frames += counts[gt_label]\n",
    "\n",
    "        total_frames = self.gt.size\n",
    "        \n",
    "        return dict(\n",
    "            pct_types_mapped=100 * len(mapped_gt_types) / len(self.gt_types) if self.gt_types.size else 0,\n",
    "            pct_frames_mapped=100 * mapped_frames / total_frames if total_frames else 0,\n",
    "            mapped_counts={k: v for k, v in counts.items() if k in mapped_gt_types},\n",
    "            unmapped_counts={k: v for k, v in counts.items() if k not in mapped_gt_types},\n",
    "        )\n",
    "\n",
    "    def plot(self, title: str = \"Clustering Evaluation\", figsize=(18, 10)) -> plt.Figure:\n",
    "        st = self.stats()\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        gs = GridSpec(2, 3, figure=fig, height_ratios=[1, 1.2])\n",
    "\n",
    "        def _annot_bar(ax, data, color, ttl):\n",
    "            if not data: # Check if data dictionary is empty\n",
    "                ax.text(0.5, 0.5, \"No data to display\", ha=\"center\", va=\"center\")\n",
    "                ax.set_axis_off()\n",
    "                return\n",
    "            \n",
    "            labels, vals = zip(*sorted(data.items())) # Sort for consistent bar order\n",
    "            \n",
    "            # Ensure vals is suitable for sum; handle potential non-numeric if data was structured unexpectedly\n",
    "            valid_vals = [v for v in vals if isinstance(v, (int, float))]\n",
    "            if not valid_vals or sum(valid_vals) == 0: # also check if sum is zero\n",
    "                perc = np.zeros_like(labels, dtype=float)\n",
    "            else:\n",
    "                perc = 100 * np.array(vals) / sum(valid_vals) # Normalize by sum of counts in this category\n",
    "\n",
    "            bars = ax.bar(range(len(labels)), perc, color=color) # Use range for x, then set_xticklabels\n",
    "            ax.set_xticks(range(len(labels)))\n",
    "            ax.set_xticklabels([str(l) for l in labels], rotation=45, ha=\"right\", fontsize=8)\n",
    "\n",
    "\n",
    "            for bar_obj, p_val in zip(bars, perc): # Renamed bar to bar_obj\n",
    "                if p_val > 0.5: # Threshold for displaying text\n",
    "                    ax.text(bar_obj.get_x() + bar_obj.get_width() / 2,\n",
    "                            p_val + 0.3, # Position text above bar\n",
    "                            f\"{p_val:.1f}%\",\n",
    "                            ha=\"center\", va=\"bottom\", fontsize=7)\n",
    "            ax.set_title(ttl)\n",
    "            ax.set_ylabel(\"% frames within this category\") # Clarified ylabel\n",
    "            ax.tick_params(axis=\"x\", rotation=45, labelsize=8)\n",
    "            ax.set_ylim(0, max(10, np.max(perc) * 1.1 if len(perc)>0 and np.max(perc) > 0 else 10)) # Dynamic Y limit\n",
    "\n",
    "        # Summary box\n",
    "        ax0 = fig.add_subplot(gs[0, 0]); ax0.axis(\"off\")\n",
    "        txt = (\n",
    "            f\"Overall FER : {self.frame_error_rate():.1f}%\\n\"\n",
    "            f\"Macro FER   : {self.macro_fer():.1f}%\\n\"\n",
    "            f\"V‑measure   : {self.v_measure():.3f}\\n\\n\"\n",
    "            f\"GT types    : {len(self.gt_types)}\\n\"\n",
    "            f\"Pred types  : {len(self.pred_types)}\\n\"\n",
    "            f\"Mapped GT types: {st['pct_types_mapped']:.1f}%\"\n",
    "        )\n",
    "        ax0.text(0.05, 0.95, txt, va=\"top\", ha=\"left\", fontsize=10, bbox=dict(fc=\"whitesmoke\", alpha=.8, boxstyle=\"round,pad=0.5\"))\n",
    "\n",
    "        # Pies for types and frames mapped\n",
    "        ax1 = fig.add_subplot(gs[0, 1])\n",
    "        if self.gt_types.size > 0 :\n",
    "             ax1.pie([st['pct_types_mapped'], 100 - st['pct_types_mapped']], labels=[\"Mapped\", \"Unmapped\"],\n",
    "                     autopct=\"%.1f%%\", colors=[\"#8fd175\", \"#f28e8e\"], startangle=90)\n",
    "        else:\n",
    "            ax1.text(0.5, 0.5, \"No GT types\", ha=\"center\", va=\"center\")\n",
    "        ax1.set_title(\"GT Label Types\")\n",
    "\n",
    "        ax2 = fig.add_subplot(gs[0, 2])\n",
    "        if self.gt.size > 0:\n",
    "            ax2.pie([st['pct_frames_mapped'], 100 - st['pct_frames_mapped']], labels=[\"Mapped\", \"Unmapped\"],\n",
    "                    autopct=\"%.1f%%\", colors=[\"#71b3ff\", \"#ffb471\"], startangle=90)\n",
    "        else:\n",
    "            ax2.text(0.5, 0.5, \"No GT frames\", ha=\"center\", va=\"center\")\n",
    "        ax2.set_title(\"GT Frames\")\n",
    "        \n",
    "        # Bars for mapped and unmapped counts\n",
    "        ax3 = fig.add_subplot(gs[1, 0:2]) # Spans two columns\n",
    "        _annot_bar(ax3, st['mapped_counts'], \"#4daf4a\", \"Mapped GT Labels (% of Mapped Frames)\")\n",
    "        \n",
    "        ax4 = fig.add_subplot(gs[1, 2]) # Single column\n",
    "        _annot_bar(ax4, st['unmapped_counts'], \"#d73027\", \"Unmapped GT Labels (% of Unmapped Frames)\")\n",
    "\n",
    "        fig.suptitle(title, fontsize=16, y=0.98) # Adjust y for suptitle\n",
    "        fig.tight_layout(rect=[0, 0, 1, 0.95]) # Adjust rect to prevent suptitle overlap\n",
    "        return fig\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Smoothing Helper Functions\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def basic_majority_vote(labels: np.ndarray, window_size: int) -> np.ndarray:\n",
    "    \"\"\"Apply basic majority vote smoothing to a 1D array of labels.\"\"\"\n",
    "    if window_size <= 1 or len(labels) == 0:\n",
    "        return labels.copy()\n",
    "    \n",
    "    n = len(labels)\n",
    "    smoothed_labels = np.copy(labels)\n",
    "    half_window = window_size // 2\n",
    "    \n",
    "    for i in range(n):\n",
    "        start = max(0, i - half_window)\n",
    "        end = min(n, i + half_window + 1)\n",
    "        window = labels[start:end]\n",
    "        \n",
    "        if len(window) > 0:\n",
    "            counts = Counter(window)\n",
    "            # Simple tie-breaking: pick the first one encountered (Python's default for Counter.most_common)\n",
    "            # or could be counts.most_common(1)[0][0]\n",
    "            # To make it slightly more stable or prefer original if it's a tie:\n",
    "            top_two = counts.most_common(2)\n",
    "            if len(top_two) == 1: # Only one item in window or all same\n",
    "                 smoothed_labels[i] = top_two[0][0]\n",
    "            elif top_two[0][1] > top_two[1][1]: # Clear winner\n",
    "                 smoothed_labels[i] = top_two[0][0]\n",
    "            else: # Tie, prefer original label if it's among the most common\n",
    "                tied_labels = [item[0] for item in top_two if item[1] == top_two[0][1]]\n",
    "                if labels[i] in tied_labels:\n",
    "                    smoothed_labels[i] = labels[i]\n",
    "                else:\n",
    "                    smoothed_labels[i] = top_two[0][0] # Fallback to the first most common\n",
    "        # If window is empty (should not happen with proper start/end), original label is kept\n",
    "            \n",
    "    return smoothed_labels\n",
    "\n",
    "def smooth_labels_per_sequence(\n",
    "    raw_labels: np.ndarray, \n",
    "    dataset_indices: np.ndarray, \n",
    "    window_size: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Apply majority vote smoothing independently to each sequence defined by dataset_indices.\"\"\"\n",
    "    if window_size <= 1:\n",
    "        return raw_labels.copy()\n",
    "    \n",
    "    smoothed_labels = np.zeros_like(raw_labels)\n",
    "    unique_indices = np.unique(dataset_indices)\n",
    "    \n",
    "    for seq_idx in unique_indices:\n",
    "        mask = (dataset_indices == seq_idx)\n",
    "        sequence_labels = raw_labels[mask]\n",
    "        if len(sequence_labels) > 0: # Ensure there are labels for this sequence\n",
    "            smoothed_labels[mask] = basic_majority_vote(sequence_labels, window_size)\n",
    "        # If sequence_labels is empty, corresponding part of smoothed_labels remains 0 or its initial value.\n",
    "            \n",
    "    return smoothed_labels\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Main Evaluation Function (Simplified)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def evaluate_predictions_with_smoothing(\n",
    "    gt_labels: np.ndarray,              # Ground truth labels\n",
    "    predicted_labels_raw: np.ndarray, # Raw predicted labels (e.g., from decoder)\n",
    "    dataset_indices: np.ndarray,      # Array indicating sequence/song boundaries for smoothing\n",
    "    output_dir: str,                  # Directory to save reports\n",
    "    smoothing_windows: List[int] = [0, 200], # Window sizes for smoothing\n",
    "    base_title: str = \"Decoder\"       # Base for plot titles\n",
    ") -> Dict[int, ClusteringMetrics]:\n",
    "    \"\"\"\n",
    "    Evaluates predicted labels against ground truth for specified smoothing windows.\n",
    "    Generates a report (plot and text summary) for each window.\n",
    "    \"\"\"\n",
    "    output_path = pathlib.Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    metrics_results_per_window: Dict[int, ClusteringMetrics] = {}\n",
    "\n",
    "    print(f\"Starting evaluation for base title: {base_title}\")\n",
    "    print(f\"Ground truth labels shape: {gt_labels.shape}\")\n",
    "    print(f\"Raw predicted labels shape: {predicted_labels_raw.shape}\")\n",
    "    print(f\"Dataset indices shape: {dataset_indices.shape}\")\n",
    "\n",
    "    for window_size in smoothing_windows:\n",
    "        print(f\"\\n--- Evaluating with smoothing window: {window_size} ---\")\n",
    "        \n",
    "        # Apply smoothing\n",
    "        if window_size == 0:\n",
    "            smoothed_predictions = predicted_labels_raw.copy()\n",
    "            print(\"Using raw predictions (no smoothing).\")\n",
    "        else:\n",
    "            if dataset_indices.size == 0:\n",
    "                print(\"Warning: dataset_indices is empty. Applying smoothing globally instead of per-sequence.\")\n",
    "                smoothed_predictions = basic_majority_vote(predicted_labels_raw, window_size)\n",
    "            elif len(np.unique(dataset_indices)) == 1 and len(dataset_indices) == len(predicted_labels_raw):\n",
    "                print(f\"Applying smoothing with window {window_size} to the whole sequence (single dataset index).\")\n",
    "                smoothed_predictions = basic_majority_vote(predicted_labels_raw, window_size)\n",
    "            else:\n",
    "                print(f\"Applying per-sequence smoothing with window {window_size}.\")\n",
    "                smoothed_predictions = smooth_labels_per_sequence(predicted_labels_raw, dataset_indices, window_size)\n",
    "        print(f\"Shape of smoothed predictions: {smoothed_predictions.shape}\")\n",
    "\n",
    "        # Calculate metrics using the ClusteringMetrics class\n",
    "        cm = ClusteringMetrics(gt_labels, smoothed_predictions)\n",
    "        metrics_results_per_window[window_size] = cm\n",
    "        \n",
    "        # --- Generate and Save Report for this window ---\n",
    "        window_report_dir = output_path / f\"{base_title.replace(' ', '_')}_window_{window_size}\"\n",
    "        window_report_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Dashboard Plot\n",
    "        plot_title = f\"{base_title} Evaluation (Smoothing Window: {window_size})\"\n",
    "        fig = cm.plot(title=plot_title)\n",
    "        plot_save_path = window_report_dir / \"metrics_dashboard.png\"\n",
    "        fig.savefig(plot_save_path, dpi=150, bbox_inches=\"tight\")\n",
    "        plt.close(fig) # Close plot to free memory\n",
    "        \n",
    "        # Text Summary\n",
    "        summary_text_path = window_report_dir / \"summary_metrics.txt\"\n",
    "        stats_data = cm.stats() # from ClusteringMetrics\n",
    "        with open(summary_text_path, \"w\") as f:\n",
    "            f.write(f\"Metrics Summary for: {base_title}\\n\")\n",
    "            f.write(f\"Smoothing Window Size: {window_size}\\n\")\n",
    "            f.write(\"-------------------------------------------------\\n\")\n",
    "            f.write(f\"V-measure Score          : {cm.v_measure():.4f}\\n\")\n",
    "            f.write(f\"Total FER               : {cm.total_fer():.2f}%\\n\")\n",
    "            f.write(f\"Matched-only FER        : {cm.matched_fer():.2f}%\\n\")\n",
    "            f.write(f\"Macro Frame Error Rate   : {cm.macro_fer():.2f}%\\n\")\n",
    "            f.write(\"-------------------------------------------------\\n\")\n",
    "            f.write(f\"GT Label Types           : {len(cm.gt_types)}\\n\")\n",
    "            f.write(f\"Predicted Label Types    : {len(cm.pred_types)}\\n\")\n",
    "            f.write(f\"% GT Types Mapped        : {stats_data['pct_types_mapped']:.2f}%\\n\")\n",
    "            f.write(f\"% GT Frames Mapped       : {stats_data['pct_frames_mapped']:.2f}%\\n\")\n",
    "            f.write(\"-------------------------------------------------\\n\")\n",
    "            f.write(\"Mapped GT Label Counts:\\n\")\n",
    "            for label, count in sorted(stats_data['mapped_counts'].items()):\n",
    "                f.write(f\"  Label {label}: {count}\\n\")\n",
    "            f.write(\"\\nUnmapped GT Label Counts:\\n\")\n",
    "            for label, count in sorted(stats_data['unmapped_counts'].items()):\n",
    "                f.write(f\"  Label {label}: {count}\\n\")\n",
    "        \n",
    "        print(f\"Saved reports for window {window_size} to: {window_report_dir}\")\n",
    "        print(f\"  V-measure: {cm.v_measure():.3f}, FER: {cm.frame_error_rate():.1f}%, Macro FER: {cm.macro_fer():.1f}%\")\n",
    "\n",
    "    # Dump all metrics to a txt file at the end\n",
    "    all_metrics_txt = output_path / \"all_metrics_dump.txt\"\n",
    "    with open(all_metrics_txt, \"w\") as f:\n",
    "        for window_size, cm in metrics_results_per_window.items():\n",
    "            stats_data = cm.stats()\n",
    "            f.write(f\"==== Window Size: {window_size} ====\\n\")\n",
    "            f.write(f\"V-measure Score          : {cm.v_measure():.4f}\\n\")\n",
    "            f.write(f\"Total FER               : {cm.total_fer():.2f}%\\n\")\n",
    "            f.write(f\"Matched-only FER        : {cm.matched_fer():.2f}%\\n\")\n",
    "            f.write(f\"Macro Frame Error Rate   : {cm.macro_fer():.2f}%\\n\")\n",
    "            f.write(f\"GT Label Types           : {len(cm.gt_types)}\\n\")\n",
    "            f.write(f\"Predicted Label Types    : {len(cm.pred_types)}\\n\")\n",
    "            f.write(f\"% GT Types Mapped        : {stats_data['pct_types_mapped']:.2f}%\\n\")\n",
    "            f.write(f\"% GT Frames Mapped       : {stats_data['pct_frames_mapped']:.2f}%\\n\")\n",
    "            f.write(\"Mapped GT Label Counts:\\n\")\n",
    "            for label, count in sorted(stats_data['mapped_counts'].items()):\n",
    "                f.write(f\"  Label {label}: {count}\\n\")\n",
    "            f.write(\"Unmapped GT Label Counts:\\n\")\n",
    "            for label, count in sorted(stats_data['unmapped_counts'].items()):\n",
    "                f.write(f\"  Label {label}: {count}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "    print(f\"Dumped all metrics to {all_metrics_txt}\")\n",
    "\n",
    "    return metrics_results_per_window\n",
    "\n",
    "import pathlib\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # iterate over all .npz result files and just print metrics\n",
    "    results_dir = pathlib.Path(\"/home/george-vengrovski/Documents/projects/tweety_bert_paper/results/decoder_eval\")\n",
    "    smoothing_windows = [0, 50, 100]\n",
    "\n",
    "    all_metrics = []\n",
    "    for results_file in results_dir.iterdir():\n",
    "        if results_file.suffix != \".npz\":\n",
    "            continue\n",
    "        print(f\"\\nprocessing {results_file.name}\")\n",
    "        try:\n",
    "            data = np.load(results_file)\n",
    "            gt = data.get(\"ground_truth_labels\")\n",
    "            pred = data.get(\"decoder_labels\")\n",
    "            if gt is None or pred is None:\n",
    "                print(\"  missing ground_truth_labels or decoder_labels, skipping\")\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(f\"  failed to load {results_file.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        for w in smoothing_windows:\n",
    "            if w == 0:\n",
    "                sm = pred.copy()\n",
    "            else:\n",
    "                sm = basic_majority_vote(pred, w)\n",
    "            cm = ClusteringMetrics(gt, sm)\n",
    "            print(\n",
    "                f\"  window {w}: \"\n",
    "                f\"v-measure={cm.v_measure():.3f}, \"\n",
    "                f\"total_fer={cm.total_fer():.2f}%, \"\n",
    "                f\"matched_fer={cm.matched_fer():.2f}%, \"\n",
    "                f\"macro_fer={cm.macro_fer():.2f}%\"\n",
    "            )\n",
    "            all_metrics.append({\n",
    "                \"file\": results_file.name,\n",
    "                \"window\": w,\n",
    "                \"v_measure\": cm.v_measure(),\n",
    "                \"total_fer\": cm.total_fer(),\n",
    "                \"matched_fer\": cm.matched_fer(),\n",
    "                \"macro_fer\": cm.macro_fer()\n",
    "            })\n",
    "\n",
    "    # Dump all metrics to a txt file\n",
    "    metrics_txt_path = results_dir / \"all_metrics_dump.txt\"\n",
    "    with open(metrics_txt_path, \"w\") as f:\n",
    "        for entry in all_metrics:\n",
    "            f.write(\n",
    "                f\"File: {entry['file']}, Window: {entry['window']}\\n\"\n",
    "                f\"  v-measure={entry['v_measure']:.3f}, \"\n",
    "                f\"total_fer={entry['total_fer']:.2f}%, \"\n",
    "                f\"matched_fer={entry['matched_fer']:.2f}%, \"\n",
    "                f\"macro_fer={entry['macro_fer']:.2f}%\\n\"\n",
    "            )\n",
    "    print(f\"Dumped all metrics to {metrics_txt_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweetybert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
