{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Data Into Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "mins_per_fold = 50\n",
    "fold_data_dir = \"/media/george-vengrovski/disk1/decoder_data\"\n",
    "\n",
    "birds_wav_paths = [\n",
    "    \"/media/george-vengrovski/disk2/canary/yarden_data/llb3_data/llb3_songs\",\n",
    "    \"/media/george-vengrovski/disk2/canary/yarden_data/llb11_data/llb11_songs\",\n",
    "    \"/media/george-vengrovski/disk2/canary/yarden_data/llb16_data/llb16_songs\"\n",
    "]\n",
    "\n",
    "song_detection_json = \"files/contains_llb.json\"\n",
    "\n",
    "# Build a mapping from filename to its full path for all birds\n",
    "wav_file_to_path = {}\n",
    "for bird_path in birds_wav_paths:\n",
    "    if os.path.isdir(bird_path):\n",
    "        for fname in os.listdir(bird_path):\n",
    "            if fname.endswith('.wav'):\n",
    "                wav_file_to_path[fname] = os.path.join(bird_path, fname)\n",
    "\n",
    "# Parse the song detection JSON and collect song files and their durations per bird\n",
    "with open(song_detection_json, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "bird_song_files = {}  # bird_id -> list of (filename, duration_seconds)\n",
    "for entry in data:\n",
    "    if not entry.get('song_present', False):\n",
    "        continue\n",
    "    filename = entry['filename']\n",
    "    if filename not in wav_file_to_path:\n",
    "        continue\n",
    "    bird_id = filename.split('_')[0]\n",
    "    total_duration = 0.0\n",
    "    for seg in entry.get('segments', []):\n",
    "        onset_ms = seg.get('onset_ms', 0)\n",
    "        offset_ms = seg.get('offset_ms', 0)\n",
    "        total_duration += (offset_ms - onset_ms) / 1000.0\n",
    "    if total_duration <= 0:\n",
    "        continue\n",
    "    if bird_id not in bird_song_files:\n",
    "        bird_song_files[bird_id] = []\n",
    "    bird_song_files[bird_id].append((filename, total_duration))\n",
    "\n",
    "# For each bird, randomly assign files to folds so each fold has at least mins_per_fold minutes\n",
    "folds_info = {}  # bird_id -> list of folds, each fold is list of (filename, duration)\n",
    "for bird_id, files in bird_song_files.items():\n",
    "    random.shuffle(files)\n",
    "    folds = []\n",
    "    current_fold = []\n",
    "    current_fold_duration = 0.0\n",
    "    for fname, dur in files:\n",
    "        current_fold.append((fname, dur))\n",
    "        current_fold_duration += dur\n",
    "        if current_fold_duration >= mins_per_fold * 60:\n",
    "            folds.append(current_fold)\n",
    "            current_fold = []\n",
    "            current_fold_duration = 0.0\n",
    "    if current_fold:  # Add any remaining files to a final fold\n",
    "        folds.append(current_fold)\n",
    "    folds_info[bird_id] = folds\n",
    "\n",
    "# Copy files to their respective fold directories with progress bar\n",
    "for bird_id, folds in folds_info.items():\n",
    "    for i, fold in enumerate(folds):\n",
    "        fold_dir = os.path.join(fold_data_dir, bird_id, f\"fold{i+1}\")\n",
    "        os.makedirs(fold_dir, exist_ok=True)\n",
    "        print(f\"Copying files for {bird_id} fold {i+1} ({len(fold)} files)...\")\n",
    "        for fname, _ in tqdm(fold, desc=f\"{bird_id} fold{i+1}\", leave=False):\n",
    "            src = wav_file_to_path[fname]\n",
    "            dst = os.path.join(fold_dir, fname)\n",
    "            if not os.path.exists(dst):\n",
    "                shutil.copy2(src, dst)\n",
    "\n",
    "# Gather fold durations for plotting\n",
    "plot_bird_ids = []\n",
    "plot_fold_names = []\n",
    "plot_fold_minutes = []\n",
    "for bird_id, folds in folds_info.items():\n",
    "    for i, fold in enumerate(folds):\n",
    "        fold_minutes = sum(dur for _, dur in fold) / 60\n",
    "        plot_bird_ids.append(bird_id)\n",
    "        plot_fold_names.append(f\"fold{i+1}\")\n",
    "        plot_fold_minutes.append(fold_minutes)\n",
    "\n",
    "# Plot bar plots showing minutes per fold for each bird\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "bar_labels = [f\"{bird}-{fold}\" for bird, fold in zip(plot_bird_ids, plot_fold_names)]\n",
    "bars = plt.bar(bar_labels, plot_fold_minutes, color='skyblue')\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.1f}',\n",
    "             ha='center', va='bottom')\n",
    "plt.title('Minutes of Song Data per Fold (per Bird)', fontsize=14, pad=20)\n",
    "plt.xlabel('Bird-Fold', fontsize=12)\n",
    "plt.ylabel('Total Duration (minutes)', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We want to create embeddings for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "\n",
    "import os\n",
    "os.environ['MKL_THREADING_LAYER'] = 'GNU'  # Add this before running your code\n",
    "\n",
    "decoding_module = importlib.import_module(\"decoding\")\n",
    "\n",
    "class Args:\n",
    "    def __init__(self, mode, bird_name, model_name, wav_folder, song_detection_json_path, num_samples_umap):\n",
    "        self.mode = mode\n",
    "        self.bird_name = bird_name\n",
    "        self.model_name = model_name\n",
    "        self.wav_folder = wav_folder\n",
    "        self.song_detection_json_path = song_detection_json_path\n",
    "        self.num_samples_umap = num_samples_umap\n",
    "        self.num_random_files_spec = 1000  # Default value\n",
    "        self.single_threaded_spec = False  # Default value\n",
    "        self.nfft = 1024  # Default value\n",
    "        self.raw_spectrogram_umap = False  # Default value for store_true flag\n",
    "        self.state_finding_algorithm_umap = \"HDBSCAN\"  # Default value\n",
    "        self.context_umap = 1000  # Default value\n",
    "\n",
    "for root, dirs, files in os.walk(fold_data_dir):\n",
    "    for dir in dirs:\n",
    "        if \"fold\" in dir:\n",
    "            bird = os.path.basename(root)\n",
    "            bird_name_fold = f\"{bird}_{dir}\"\n",
    "            wav_folder = os.path.join(root, dir)\n",
    "            args = Args(\n",
    "                mode=\"single\",\n",
    "                bird_name=bird_name_fold,\n",
    "                model_name=\"BF_Canary_Joint_Run\",\n",
    "                wav_folder=wav_folder,\n",
    "                song_detection_json_path=song_detection_json,\n",
    "                num_samples_umap=\"1e6\"\n",
    "            )\n",
    "            print(f\"Running decoding.py --mode single --bird_name {bird_name_fold} --model_name BF_Canary_Joint_Run --wav_folder {wav_folder} --song_detection_json_path {song_detection_json} --num_samples_umap 1e6\")\n",
    "            decoding_module.main(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Rapids Env, Generate UMAP embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/george-vengrovski/.local/share/mamba/envs/rapids-25.04/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting teacher label generation for ALL folds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Folds:  93%|█████████▎| 14/15 [02:16<00:09,  9.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-09 20:20:30.813] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
      "[2025-06-09 20:20:30.814] [CUML] [info] Building knn graph using nn descent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Folds: 100%|██████████| 15/15 [04:34<00:00, 18.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher label generation complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from cuml.manifold import UMAP as UMAP_cuml\n",
    "from cuml.cluster import HDBSCAN as HDBSCAN_cuml\n",
    "from scipy.stats import mode\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def majority_vote(data, window_size=1):\n",
    "    \"\"\"\n",
    "    Return an array of the same length as 'data',\n",
    "    where each index i is replaced by the majority over\n",
    "    a window around i. No padding is added.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from collections import Counter\n",
    "\n",
    "    data = np.asarray(data)\n",
    "    n = len(data)\n",
    "    \n",
    "    # If window_size=1, no smoothing\n",
    "    if window_size <= 1 or n == 0:\n",
    "        return data.copy()\n",
    "    \n",
    "    half_w = window_size // 2\n",
    "    output = np.zeros_like(data)\n",
    "    \n",
    "    for i in range(n):\n",
    "        # define start/end, clamped\n",
    "        start = max(0, i - half_w)\n",
    "        end   = min(n, i + half_w + 1)\n",
    "        window = data[start:end]\n",
    "        \n",
    "        # majority\n",
    "        c = Counter(window)\n",
    "        major_label = max(c, key=c.get)  # picks the label with highest freq\n",
    "        output[i] = major_label\n",
    "    \n",
    "    return output\n",
    "\n",
    "def fill_noise_with_nearest_label(labels):\n",
    "    noise_indices = np.where(labels == -1)[0]\n",
    "    non_noise_indices = np.where(labels != -1)[0]\n",
    "    if len(noise_indices) == 0 or len(non_noise_indices) == 0: return labels\n",
    "    right_neighbor_idxs = np.searchsorted(non_noise_indices, noise_indices)\n",
    "    left_neighbor_idxs = right_neighbor_idxs - 1\n",
    "    right_neighbor_idxs[right_neighbor_idxs == len(non_noise_indices)] = len(non_noise_indices) - 1\n",
    "    dist_right = np.abs(non_noise_indices[right_neighbor_idxs] - noise_indices).astype(float)\n",
    "    dist_left = np.abs(noise_indices - non_noise_indices[left_neighbor_idxs]).astype(float)\n",
    "    dist_left[left_neighbor_idxs < 0] = np.inf\n",
    "    winner_indices = np.where(dist_right <= dist_left, non_noise_indices[right_neighbor_idxs], non_noise_indices[left_neighbor_idxs])\n",
    "    labels[noise_indices] = labels[winner_indices]\n",
    "    return labels\n",
    "\n",
    "# --- Constants ---\n",
    "ROOT = pathlib.Path().resolve()\n",
    "NPZ_DIR = ROOT / \"files\"\n",
    "PROCESSED_DIR = ROOT / \"files_with_teacher_labels\"\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ALL_FILES_TO_PROCESS = [\n",
    "    f'{bird}_fold{i}.npz' for bird in ['llb3', 'llb11', 'llb16'] for i in range(1, 6)\n",
    "]\n",
    "MAX_FRAMES = 1_000_000\n",
    "\n",
    "# --- Main Processing Loop ---\n",
    "print(f\"Starting teacher label generation for ALL folds...\")\n",
    "for fname in tqdm(ALL_FILES_TO_PROCESS, desc=\"Processing Folds\"):\n",
    "    in_path = NPZ_DIR / fname\n",
    "    out_path = PROCESSED_DIR / fname\n",
    "\n",
    "    if out_path.exists():\n",
    "        continue\n",
    "\n",
    "    f = np.load(in_path, allow_pickle=True)\n",
    "\n",
    "    predictions = f['predictions'][:MAX_FRAMES]\n",
    "    vocalization = f['vocalization'][:MAX_FRAMES]\n",
    "    ground_truth_labels = f['ground_truth_labels'][:MAX_FRAMES]\n",
    "    s = f['s'][:MAX_FRAMES, :]\n",
    "\n",
    "    # Initialize variables to None for proper cleanup\n",
    "    emb_eval_gpu = None\n",
    "    z_umap = None\n",
    "    umap_cuml = None\n",
    "    hdbscan_cuml = None\n",
    "\n",
    "\n",
    "    emb_eval_gpu = cp.asarray(predictions)\n",
    "    umap_cuml = UMAP_cuml(n_neighbors=50, n_components=8, min_dist=0.10, metric=\"euclidean\", init=\"spectral\", n_epochs=200, n_jobs=-1)\n",
    "    z_umap = umap_cuml.fit_transform(emb_eval_gpu)\n",
    "    hdbscan_cuml = HDBSCAN_cuml(min_cluster_size=2500, min_samples=250, prediction_data=True)\n",
    "    raw_teacher_labels = hdbscan_cuml.fit_predict(z_umap).get()\n",
    "\n",
    "    # Aggressive cleanup of GPU memory\n",
    "    if emb_eval_gpu is not None:\n",
    "        del emb_eval_gpu\n",
    "    if z_umap is not None:\n",
    "        del z_umap\n",
    "    if umap_cuml is not None:\n",
    "        del umap_cuml\n",
    "    if hdbscan_cuml is not None:\n",
    "        del hdbscan_cuml\n",
    "    \n",
    "    # Force GPU memory cleanup\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "    cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "\n",
    "    filled_labels = fill_noise_with_nearest_label(raw_teacher_labels)\n",
    "\n",
    "    teacher_labels = majority_vote(filled_labels, window_size=200)\n",
    "\n",
    "    np.savez_compressed(out_path, predictions=predictions, hdbscan_labels=teacher_labels, vocalization=vocalization, ground_truth_labels=ground_truth_labels, s=s)\n",
    "    \n",
    "    # Additional cleanup after saving\n",
    "    del f, predictions, vocalization, ground_truth_labels, s, raw_teacher_labels, filled_labels, teacher_labels\n",
    "    gc.collect()\n",
    "\n",
    "print(\"Teacher label generation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Training and Evalulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes (eval notebook): 23\n",
      "[resume] Skipping llb3 | Train: llb3_fold1.npz | Decoder: linear_probe\n",
      "[resume] Skipping llb3 | Train: llb3_fold1.npz | Decoder: mlp_unweighted\n",
      "[resume] Skipping llb3 | Train: llb3_fold1.npz | Decoder: mlp_weighted\n",
      "Number of classes (eval notebook): 20\n",
      "[resume] Skipping llb3 | Train: llb3_fold2.npz | Decoder: linear_probe\n",
      "[resume] Skipping llb3 | Train: llb3_fold2.npz | Decoder: mlp_unweighted\n",
      "[resume] Skipping llb3 | Train: llb3_fold2.npz | Decoder: mlp_weighted\n",
      "Number of classes (eval notebook): 22\n",
      "[resume] Skipping llb3 | Train: llb3_fold3.npz | Decoder: linear_probe\n",
      "[resume] Skipping llb3 | Train: llb3_fold3.npz | Decoder: mlp_unweighted\n",
      "[resume] Skipping llb3 | Train: llb3_fold3.npz | Decoder: mlp_weighted\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "import shutil\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --- Constants ---\n",
    "ROOT = pathlib.Path().resolve()\n",
    "NPZ_DIR = ROOT / \"files_with_teacher_labels\"\n",
    "SAVE_DIR = ROOT / \"results\" / \"decoder_avg_perf_eval\"\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CSV_PATH = SAVE_DIR / \"evaluation_results.csv\"\n",
    "\n",
    "# Define train/test files for each bird\n",
    "ALL_FILES = {\n",
    "    'llb3': [f'llb3_fold{i}.npz' for i in range(1, 6)],\n",
    "    'llb11': [f'llb11_fold{i}.npz' for i in range(1, 6)],\n",
    "    'llb16': [f'llb16_fold{i}.npz' for i in range(1, 6)]\n",
    "}\n",
    "TEST_FOLD_NUM = 5\n",
    "\n",
    "MAX_FRAMES = 1_000_000\n",
    "CTX = 1_000\n",
    "DEV = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_DIR = ROOT / \"experiments\" / \"TweetyBERT_Paper_Yarden_Model\"\n",
    "\n",
    "# --- helpers: i/o + tiny utils ---\n",
    "def load_npz(fp: pathlib.Path) -> Tuple[np.ndarray, ...]:\n",
    "    with np.load(fp) as f:\n",
    "        return (\n",
    "            f[\"s\"][:MAX_FRAMES],\n",
    "            f[\"hdbscan_labels\"][:MAX_FRAMES],\n",
    "            f[\"ground_truth_labels\"][:MAX_FRAMES],\n",
    "        )\n",
    "\n",
    "# --- helpers: decoder forward ---\n",
    "sys.path.insert(0, str(pathlib.Path(\"src\").resolve()))\n",
    "from src.decoder import TweetyBertClassifier,SongDataSet_Image, CollateFunction\n",
    "\n",
    "## For evalulation of decoder performance\n",
    "def run_decoder(model_torch: torch.nn.Module,\n",
    "                specs: np.ndarray,\n",
    "                gt: np.ndarray,\n",
    "                tmp_dir: pathlib.Path) -> Tuple[np.ndarray, float, np.ndarray]:\n",
    "    tmp_dir.mkdir(parents=True, exist_ok=True)\n",
    "    seg_id = 0\n",
    "    for start in range(0, len(specs), CTX):\n",
    "        seg = specs[start:start + CTX]\n",
    "        seg = np.pad(seg, ((0, 0), (20, 0)), constant_values=0)\n",
    "        if seg.shape[0] < CTX:\n",
    "            seg = np.pad(seg, ((0, CTX - seg.shape[0]), (0, 0)))\n",
    "        gt_seg = gt[start:start + CTX]\n",
    "        if gt_seg.shape[0] < CTX:\n",
    "            gt_seg = np.pad(gt_seg, (0, CTX - gt_seg.shape[0]))\n",
    "        # write the slice so the dataset isn't empty\n",
    "        np.savez(\n",
    "            tmp_dir / f\"{seg_id}.npz\",\n",
    "            labels=gt_seg,\n",
    "            s=seg.T,                        # freq × time, per decoder expectations\n",
    "            vocalization=np.zeros(CTX, dtype=np.int8)\n",
    "        )\n",
    "        seg_id += 1\n",
    "\n",
    "    if seg_id == 0:\n",
    "        raise RuntimeError(\"no segments saved – tmp_dir empty\")\n",
    "\n",
    "    ds = SongDataSet_Image(tmp_dir,\n",
    "                           num_classes=int(gt.max()) + 1,\n",
    "                           segment_length=CTX,\n",
    "                           infinite_loader=False)\n",
    "    dl = DataLoader(ds, batch_size=1, shuffle=False,\n",
    "                    collate_fn=CollateFunction(segment_length=CTX))\n",
    "\n",
    "    preds, logits_accum, gts_accum = [], [], []\n",
    "    t0 = time.perf_counter()\n",
    "    model_torch.eval()\n",
    "    with torch.no_grad():\n",
    "        for b in dl:\n",
    "            s = b[0].to(DEV)           # first element = spectrogram tensor\n",
    "            gt = b[1].to(DEV)\n",
    "            logits = model_torch(s)          # (1, S, C)\n",
    "            preds.append(torch.argmax(logits, 2).cpu().numpy())\n",
    "            logits_accum.append(logits.cpu().numpy())        # (1, S, C)\n",
    "            gts_accum.append(torch.argmax(gt, 2).cpu().numpy())\n",
    "\n",
    "    t_elapsed = time.perf_counter() - t0\n",
    "\n",
    "    shutil.rmtree(tmp_dir, ignore_errors=True)\n",
    "    preds   = np.concatenate([p.squeeze(0) for p in preds])[:len(specs)]\n",
    "    logits  = np.concatenate([l.squeeze(0) for l in logits_accum])[:len(specs)]\n",
    "    gts     = np.concatenate([g.squeeze(0) for g in gts_accum])[:len(specs)]\n",
    "\n",
    "    return preds, t_elapsed, logits, gts\n",
    "\n",
    "# --- Main benchmark function ---\n",
    "def benchmark_bird(bird_id: str, train_files: List[str], test_file: str) -> None:\n",
    "    csv_rows = []\n",
    "    if CSV_PATH.exists():\n",
    "        _done_df = pd.read_csv(CSV_PATH)\n",
    "        # Resumption key includes the specific training fold\n",
    "        done_keys = {(r.bird, r.train_fold, r.decoder_type) for _, r in _done_df.iterrows()}\n",
    "    else:\n",
    "        done_keys = set()\n",
    "\n",
    "    # --- 1. Iterate through each training fold ---\n",
    "    for fit_name in train_files:\n",
    "        fit_path = NPZ_DIR / fit_name\n",
    "\n",
    "        # --- 2. Define and loop through decoder configurations ---\n",
    "        decoder_configs = [\n",
    "            {'name': 'linear_probe', 'classifier_type': 'linear_probe', 'use_weights': False},\n",
    "            {'name': 'mlp_unweighted', 'classifier_type': 'decoder', 'use_weights': False},\n",
    "            {'name': 'mlp_weighted', 'classifier_type': 'decoder', 'use_weights': True},\n",
    "        ]\n",
    "\n",
    "        # Calculate class weights based on the current training fold\n",
    "        _, teacher_labels, _ = load_npz(fit_path)\n",
    "\n",
    "        # Then, calculate num_classes based on the valid labels\n",
    "        num_classes = int(teacher_labels.max()) + 1 if teacher_labels.size > 0 else 1\n",
    "        print(f\"Number of classes (eval notebook): {num_classes}\")\n",
    "\n",
    "        # Create a Counter object to efficiently count label occurrences\n",
    "        label_counts = Counter(teacher_labels)\n",
    "\n",
    "        # Use the Counter object's .get() method to build the class_counts array\n",
    "        class_counts = np.array([label_counts.get(i, 0) for i in range(num_classes)])\n",
    "        class_counts[class_counts == 0] = 1\n",
    "        _class_weights = 1.0 / class_counts\n",
    "        _class_weights /= _class_weights.sum() / num_classes\n",
    "        _class_weights = torch.tensor(_class_weights, dtype=torch.float32, device=DEV)\n",
    "\n",
    "        for config in decoder_configs:\n",
    "            decoder_type = config['name']\n",
    "            if (bird_id, fit_name, decoder_type) in done_keys:\n",
    "                print(f\"[resume] Skipping {bird_id} | Train: {fit_name} | Decoder: {decoder_type}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"--- Processing {bird_id} | Train: {fit_name} | Decoder: {decoder_type} ---\")\n",
    "\n",
    "            # --- 3. Train decoder on the single training fold ---\n",
    "            dec_dir = SAVE_DIR / f\"{bird_id}__{fit_name}__{decoder_type}_decoder\"\n",
    "            probe = TweetyBertClassifier(\n",
    "                model_dir=str(MODEL_DIR),\n",
    "                linear_decoder_dir=str(dec_dir),\n",
    "                context_length=CTX,\n",
    "                classifier_type=config['classifier_type'],\n",
    "                weight=_class_weights if config['use_weights'] else None\n",
    "            )\n",
    "            probe.prepare_data(str(fit_path), test_train_split=0.8)\n",
    "            probe.create_dataloaders(batch_size=128)\n",
    "            probe.create_classifier()\n",
    "            probe.train_classifier(lr=1e-3, desired_total_batches=1000, batches_per_eval=50, patience=4)\n",
    "            probe_model = probe.classifier_model.to(DEV)\n",
    "\n",
    "            # --- 4. Evaluate on the single held-out test fold ---\n",
    "            print(f\"Evaluating on test fold: {test_file}\")\n",
    "            specs_eval, _, gt_eval = load_npz(NPZ_DIR / test_file)\n",
    "            preds, t_elapsed, logits, gts = run_decoder(probe_model, specs_eval, gt_eval, SAVE_DIR / \"tmp_eval\")\n",
    "\n",
    "            res_path = SAVE_DIR / f\"{bird_id}_{fit_name}_{decoder_type}.results.npz\"\n",
    "            np.savez_compressed(res_path, decoder_labels=preds, ground_truth_labels=gts)\n",
    "            \n",
    "            csv_rows.append(dict(\n",
    "                bird=bird_id,\n",
    "                train_fold=fit_name, # Log which fold was used for training\n",
    "                decoder_type=decoder_type,\n",
    "                eval_fold=test_file,\n",
    "                decoder_s_per_row=t_elapsed / len(specs_eval) if len(specs_eval) > 0 else 0,\n",
    "                results_path=str(res_path)\n",
    "            ))\n",
    "\n",
    "            shutil.rmtree(dec_dir, ignore_errors=True)\n",
    "            probe_model.to('cpu'); del probe_model, probe; torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "    # --- 5. Log results ---\n",
    "    if csv_rows:\n",
    "        pd.DataFrame(csv_rows).to_csv(CSV_PATH, mode=\"a\", header=not CSV_PATH.exists(), index=False)\n",
    "\n",
    "# --- driver ---\n",
    "if __name__ == \"__main__\":\n",
    "    for bird_id, files in ALL_FILES.items():\n",
    "        train_files = [f for f in files if str(TEST_FOLD_NUM) not in f]\n",
    "        test_file = f\"{bird_id}_fold{TEST_FOLD_NUM}.npz\"\n",
    "        \n",
    "        if not (NPZ_DIR / test_file).exists():\n",
    "            print(f\"Warning: Test file {test_file} not found in {NPZ_DIR}. Skipping bird {bird_id}.\")\n",
    "            continue\n",
    "            \n",
    "        benchmark_bird(bird_id, train_files, test_file)\n",
    "        \n",
    "    if CSV_PATH.exists():\n",
    "        df = pd.read_csv(CSV_PATH)\n",
    "        print(\"\\n--- Average Performance Across Training Folds (Mean sec / frame) ---\")\n",
    "        # To get the average performance, group by bird and decoder_type, and average the results from each train_fold\n",
    "        print(df.groupby([\"bird\", \"decoder_type\"])[[\"decoder_s_per_row\"]].mean())\n",
    "    else:\n",
    "        print(\"\\nNo evaluation results generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "processing llb3_llb3_fold2.npz_mlp_weighted.results.npz\n",
      "  window 0: v-measure=0.606, total_fer=21.60%, matched_fer=16.37%, macro_fer=52.71%\n",
      "  window 50: v-measure=0.642, total_fer=20.45%, matched_fer=14.74%, macro_fer=51.39%\n",
      "  window 100: v-measure=0.651, total_fer=20.02%, matched_fer=14.27%, macro_fer=51.09%\n",
      "\n",
      "processing llb3_llb3_fold4.npz_mlp_unweighted.results.npz\n",
      "  window 0: v-measure=0.559, total_fer=21.83%, matched_fer=18.95%, macro_fer=54.49%\n",
      "  window 50: v-measure=0.646, total_fer=18.86%, matched_fer=14.21%, macro_fer=48.74%\n",
      "  window 100: v-measure=0.644, total_fer=18.82%, matched_fer=14.16%, macro_fer=49.51%\n",
      "\n",
      "processing llb3_llb3_fold3.npz_mlp_weighted.results.npz\n",
      "  window 0: v-measure=0.565, total_fer=44.40%, matched_fer=43.54%, macro_fer=35.11%\n",
      "  window 50: v-measure=0.618, total_fer=42.09%, matched_fer=41.19%, macro_fer=31.47%\n",
      "  window 100: v-measure=0.619, total_fer=41.87%, matched_fer=40.96%, macro_fer=30.68%\n",
      "\n",
      "processing llb3_llb3_fold1.npz_mlp_weighted.results.npz\n",
      "  window 0: v-measure=0.598, total_fer=34.35%, matched_fer=32.75%, macro_fer=43.42%\n",
      "  window 50: v-measure=0.644, total_fer=32.99%, matched_fer=31.36%, macro_fer=41.20%\n",
      "  window 100: v-measure=0.651, total_fer=32.52%, matched_fer=30.88%, macro_fer=40.79%\n",
      "\n",
      "processing llb3_llb3_fold1.npz_mlp_unweighted.results.npz\n",
      "  window 0: v-measure=0.559, total_fer=34.49%, matched_fer=33.23%, macro_fer=52.70%\n",
      "  window 50: v-measure=0.621, total_fer=32.57%, matched_fer=31.25%, macro_fer=46.81%\n",
      "  window 100: v-measure=0.620, total_fer=32.70%, matched_fer=31.40%, macro_fer=47.80%\n",
      "\n",
      "processing llb3_llb3_fold2.npz_mlp_unweighted.results.npz\n",
      "  window 0: v-measure=0.567, total_fer=21.54%, matched_fer=16.36%, macro_fer=60.94%\n",
      "  window 50: v-measure=0.618, total_fer=20.44%, matched_fer=11.55%, macro_fer=53.21%\n",
      "  window 100: v-measure=0.620, total_fer=20.40%, matched_fer=11.52%, macro_fer=53.31%\n",
      "\n",
      "processing llb3_llb3_fold2.npz_linear_probe.results.npz\n",
      "  window 0: v-measure=0.272, total_fer=39.37%, matched_fer=35.35%, macro_fer=72.28%\n",
      "  window 50: v-measure=0.408, total_fer=34.50%, matched_fer=30.04%, macro_fer=70.06%\n",
      "  window 100: v-measure=0.406, total_fer=35.36%, matched_fer=30.03%, macro_fer=71.46%\n",
      "\n",
      "processing llb3_llb3_fold4.npz_mlp_weighted.results.npz\n",
      "  window 0: v-measure=0.658, total_fer=17.14%, matched_fer=14.08%, macro_fer=37.66%\n",
      "  window 50: v-measure=0.704, total_fer=15.37%, matched_fer=12.25%, macro_fer=34.09%\n",
      "  window 100: v-measure=0.711, total_fer=15.14%, matched_fer=12.01%, macro_fer=33.74%\n",
      "\n",
      "processing llb3_llb3_fold4.npz_linear_probe.results.npz\n",
      "  window 0: v-measure=0.372, total_fer=33.04%, matched_fer=30.56%, macro_fer=65.57%\n",
      "  window 50: v-measure=0.501, total_fer=26.82%, matched_fer=20.42%, macro_fer=59.26%\n",
      "  window 100: v-measure=0.525, total_fer=25.03%, matched_fer=18.48%, macro_fer=58.75%\n",
      "\n",
      "processing llb3_llb3_fold3.npz_linear_probe.results.npz\n",
      "  window 0: v-measure=0.435, total_fer=39.16%, matched_fer=38.21%, macro_fer=51.63%\n",
      "  window 50: v-measure=0.570, total_fer=59.74%, matched_fer=58.08%, macro_fer=41.69%\n",
      "  window 100: v-measure=0.579, total_fer=59.84%, matched_fer=58.19%, macro_fer=41.04%\n",
      "\n",
      "processing llb3_llb3_fold3.npz_mlp_unweighted.results.npz\n",
      "  window 0: v-measure=0.547, total_fer=62.13%, matched_fer=61.54%, macro_fer=49.92%\n",
      "  window 50: v-measure=0.626, total_fer=61.77%, matched_fer=61.36%, macro_fer=46.12%\n",
      "  window 100: v-measure=0.629, total_fer=62.01%, matched_fer=60.87%, macro_fer=47.01%\n",
      "\n",
      "processing llb3_llb3_fold1.npz_linear_probe.results.npz\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 498\u001b[0m\n\u001b[1;32m    493\u001b[0m             sm \u001b[38;5;241m=\u001b[39m basic_majority_vote(pred, w)\n\u001b[1;32m    494\u001b[0m         cm \u001b[38;5;241m=\u001b[39m ClusteringMetrics(gt, sm)\n\u001b[1;32m    495\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    496\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  window \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    497\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv-measure=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcm\u001b[38;5;241m.\u001b[39mv_measure()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 498\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_fer=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcm\u001b[38;5;241m.\u001b[39mtotal_fer()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    499\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatched_fer=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcm\u001b[38;5;241m.\u001b[39mmatched_fer()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    500\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmacro_fer=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcm\u001b[38;5;241m.\u001b[39mmacro_fer()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    501\u001b[0m         )\n\u001b[1;32m    502\u001b[0m         all_metrics\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m    503\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m: results_file\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    504\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow\u001b[39m\u001b[38;5;124m\"\u001b[39m: w,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmacro_fer\u001b[39m\u001b[38;5;124m\"\u001b[39m: cm\u001b[38;5;241m.\u001b[39mmacro_fer()\n\u001b[1;32m    509\u001b[0m         })\n\u001b[1;32m    511\u001b[0m \u001b[38;5;66;03m# Dump all metrics to a txt file\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 161\u001b[0m, in \u001b[0;36mClusteringMetrics.total_fer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtotal_fer\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m    160\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"error rate across *all* frames (current behaviour).\"\"\"\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fer_generic()\n",
      "Cell \u001b[0;32mIn[4]\u001b[0m, in \u001b[0;36mClusteringMetrics._fer_generic\u001b[0;34m(self, use_mask)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics import v_measure_score\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "from collections import Counter\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# ClusteringMetrics Class (calculates metrics and generates dashboard plot)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "class ClusteringMetrics:\n",
    "    \"\"\"Evaluate clustering vs. ground‑truth phrase labels.\"\"\"\n",
    "\n",
    "    def __init__(self, gt: np.ndarray, pred: np.ndarray, silence: int = 0):\n",
    "        if gt.shape != pred.shape:\n",
    "            # Try to truncate the longer array if lengths differ by a small margin\n",
    "            # This can happen if pred_dec was based on spec length and gt was slightly different\n",
    "            min_len = min(len(gt), len(pred))\n",
    "            if abs(len(gt) - len(pred)) > 100: # Arbitrary threshold for \"small margin\"\n",
    "                 raise ValueError(f\"gt (shape {gt.shape}) and pred (shape {pred.shape}) arrays must have identical shape or be very close.\")\n",
    "            print(f\"Warning: GT and Pred shapes differ ({gt.shape} vs {pred.shape}). Truncating to shortest length: {min_len}\")\n",
    "            gt = gt[:min_len]\n",
    "            pred = pred[:min_len]\n",
    "\n",
    "        self.gt_raw = gt.astype(int)\n",
    "        self.pred = pred.astype(int)\n",
    "        self.gt = self._merge_silence(self.gt_raw, silence) # Processed GT\n",
    "\n",
    "        self.gt_types = np.unique(self.gt)\n",
    "        self.pred_types = np.unique(self.pred)\n",
    "\n",
    "        self._build_confusion()\n",
    "        self.mapping = self._hungarian()\n",
    "\n",
    "    @staticmethod\n",
    "    def _merge_silence(arr: np.ndarray, silence: int) -> np.ndarray:\n",
    "        \"\"\"Fill contiguous *silence* runs with the nearest neighbour label.\"\"\"\n",
    "        if arr.size == 0:\n",
    "            return arr\n",
    "        out = arr.copy()\n",
    "        i = 0\n",
    "        while i < len(out):\n",
    "            if out[i] != silence:\n",
    "                i += 1\n",
    "                continue\n",
    "            j = i\n",
    "            while j < len(out) and out[j] == silence:\n",
    "                j += 1\n",
    "            \n",
    "            # Determine fill value\n",
    "            # Prefer left non-silence, then right, then keep silence if surrounded\n",
    "            left_val = out[i-1] if i > 0 and out[i-1] != silence else None\n",
    "            right_val = out[j] if j < len(out) and out[j] != silence else None\n",
    "\n",
    "            if left_val is not None:\n",
    "                fill = left_val\n",
    "            elif right_val is not None:\n",
    "                fill = right_val\n",
    "            else: # Both neighbors are silence or out of bounds\n",
    "                # If it's an isolated block of silence or all silence, keep as silence\n",
    "                # Or, if you have a default non-silence label, use that.\n",
    "                # For now, if no non-silence neighbor, it will effectively extend the last seen non-silence or first seen.\n",
    "                # A better strategy might be needed if all is silence or starts/ends with long silence.\n",
    "                # Using a simpler logic: if left exists, use it, else if right exists, use it, else keep silence.\n",
    "                left_neighbor = out[i - 1] if i > 0 else None\n",
    "                right_neighbor = out[j] if j < len(out) else None\n",
    "                \n",
    "                if left_neighbor is not None and left_neighbor != silence:\n",
    "                    fill = left_neighbor\n",
    "                elif right_neighbor is not None and right_neighbor != silence:\n",
    "                    fill = right_neighbor\n",
    "                else: # if surrounded by silence or at edges with silence\n",
    "                    # find first non-silence from start if available\n",
    "                    first_nonsilence_overall = next((val for val in arr if val != silence), silence)\n",
    "                    fill = first_nonsilence_overall\n",
    "\n",
    "            out[i:j] = fill\n",
    "            i = j\n",
    "        return out\n",
    "\n",
    "    def _build_confusion(self) -> None:\n",
    "        \"\"\"GT×Pred frame counts + column‑normalised version.\"\"\"\n",
    "        if not self.gt_types.size or not self.pred_types.size:\n",
    "            self.C = np.array([], dtype=int)\n",
    "            self.C_norm = np.array([], dtype=float)\n",
    "            return\n",
    "\n",
    "        gt_idx = {l: i for i, l in enumerate(self.gt_types)}\n",
    "        pr_idx = {l: i for i, l in enumerate(self.pred_types)}\n",
    "        self.C = np.zeros((len(self.gt_types), len(self.pred_types)), dtype=int)\n",
    "        \n",
    "        for g_val, p_val in zip(self.gt, self.pred):\n",
    "            if g_val in gt_idx and p_val in pr_idx: # Ensure labels are in unique sets\n",
    "                 np.add.at(self.C, (gt_idx[g_val], pr_idx[p_val]), 1)\n",
    "        \n",
    "        col_sum = self.C.sum(axis=0, keepdims=True)\n",
    "        with np.errstate(divide='ignore', invalid='ignore'): # Handle division by zero for columns with no predictions\n",
    "            self.C_norm = np.divide(self.C, col_sum, where=col_sum != 0, out=np.zeros_like(self.C, dtype=float))\n",
    "\n",
    "\n",
    "    def _hungarian(self) -> Dict[int, int]:\n",
    "        if not hasattr(self, 'C_norm') or self.C_norm.size == 0:\n",
    "            return {}\n",
    "        # Cost matrix for Hungarian algorithm should maximize overlap, so use negative C_norm\n",
    "        cost_matrix = -self.C_norm\n",
    "        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "        \n",
    "        mapping = {}\n",
    "        # Ensure indices are within bounds\n",
    "        for r, c in zip(row_ind, col_ind):\n",
    "            if r < len(self.gt_types) and c < len(self.pred_types):\n",
    "                mapping[self.gt_types[r]] = self.pred_types[c]\n",
    "        return mapping\n",
    "\n",
    "    def v_measure(self) -> float:\n",
    "        if self.gt.size == 0 or self.pred.size == 0:\n",
    "            return 0.0\n",
    "        # V-measure can't handle if one of the arrays is all one label and the other is different\n",
    "        # Or if arrays are empty after potential initial processing.\n",
    "        try:\n",
    "            return v_measure_score(self.gt, self.pred)\n",
    "        except ValueError:\n",
    "             # This can happen if, for example, gt or pred contains only one unique label\n",
    "             # and it doesn't align in a way v_measure_score expects, or if labels are negative.\n",
    "             # A simple check: if number of unique labels is 1 for both and they are same, V is 1, else 0.\n",
    "            if len(self.gt_types) == 1 and len(self.pred_types) == 1:\n",
    "                return 1.0 if self.gt_types[0] == self.pred_types[0] else 0.0\n",
    "            return 0.0 # Default for other ValueErrors\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # three flavours of frame-error-rate\n",
    "    # ------------------------------------------------------------------\n",
    "    def _fer_generic(\n",
    "        self,\n",
    "        use_mask: Optional[np.ndarray] = None\n",
    "    ) -> float:\n",
    "        \"\"\"helper: compute fer over the whole array or a masked subset.\"\"\"\n",
    "        if self.gt.size == 0:\n",
    "            return 0.0\n",
    "\n",
    "        if use_mask is None:\n",
    "            use_mask = np.ones_like(self.gt, dtype=bool)\n",
    "\n",
    "        masked_gt   = self.gt[use_mask]\n",
    "        masked_pred = self.pred[use_mask]\n",
    "\n",
    "        if masked_gt.size == 0:\n",
    "            return 0.0\n",
    "\n",
    "        correct = 0\n",
    "        for g, p in zip(masked_gt, masked_pred):\n",
    "            if g in self.mapping and self.mapping[g] == p:\n",
    "                correct += 1\n",
    "        return 100.0 * (1.0 - correct / masked_gt.size)\n",
    "\n",
    "    def total_fer(self) -> float:\n",
    "        \"\"\"error rate across *all* frames (current behaviour).\"\"\"\n",
    "        return self._fer_generic()\n",
    "\n",
    "    def matched_fer(self) -> float:\n",
    "        \"\"\"error rate restricted to frames whose GT label is mapped.\"\"\"\n",
    "        mapped_mask = np.isin(self.gt, list(self.mapping.keys()))\n",
    "        return self._fer_generic(use_mask=mapped_mask)\n",
    "\n",
    "    # keep old name as alias for backwards compat\n",
    "    def frame_error_rate(self) -> float:\n",
    "        return self.total_fer()\n",
    "\n",
    "    def macro_fer(self) -> float:\n",
    "        if not self.gt_types.size:\n",
    "            return 0.0\n",
    "        \n",
    "        per_type_fer = []\n",
    "        for gt_label_type in self.gt_types:\n",
    "            # Frames corresponding to this ground truth type\n",
    "            type_mask = (self.gt == gt_label_type)\n",
    "            if not np.any(type_mask): # No frames for this GT type\n",
    "                continue\n",
    "\n",
    "            # If this GT type is not in the mapping, all its frames are errors for Macro FER\n",
    "            if gt_label_type not in self.mapping:\n",
    "                per_type_fer.append(1.0) # 100% error for this unmapped type\n",
    "                continue\n",
    "\n",
    "            mapped_pred_label = self.mapping[gt_label_type]\n",
    "            \n",
    "            # Calculate errors for this type\n",
    "            errors_for_type = np.sum(self.pred[type_mask] != mapped_pred_label)\n",
    "            total_for_type = np.sum(type_mask)\n",
    "            \n",
    "            per_type_fer.append(errors_for_type / total_for_type if total_for_type > 0 else 0.0)\n",
    "            \n",
    "        return 100.0 * np.mean(per_type_fer) if per_type_fer else 0.0\n",
    "\n",
    "    def stats(self) -> Dict[str, Any]:\n",
    "        if self.gt.size == 0: # Handle empty case\n",
    "            return dict(\n",
    "                pct_types_mapped=0, pct_frames_mapped=0,\n",
    "                mapped_counts={}, unmapped_counts={},\n",
    "            )\n",
    "        counts = {g: (self.gt == g).sum() for g in self.gt_types}\n",
    "        mapped_gt_types = set(self.mapping.keys())\n",
    "        \n",
    "        mapped_frames = 0\n",
    "        for gt_label in mapped_gt_types:\n",
    "            if gt_label in counts:\n",
    "                mapped_frames += counts[gt_label]\n",
    "\n",
    "        total_frames = self.gt.size\n",
    "        \n",
    "        return dict(\n",
    "            pct_types_mapped=100 * len(mapped_gt_types) / len(self.gt_types) if self.gt_types.size else 0,\n",
    "            pct_frames_mapped=100 * mapped_frames / total_frames if total_frames else 0,\n",
    "            mapped_counts={k: v for k, v in counts.items() if k in mapped_gt_types},\n",
    "            unmapped_counts={k: v for k, v in counts.items() if k not in mapped_gt_types},\n",
    "        )\n",
    "\n",
    "    def plot(self, title: str = \"Clustering Evaluation\", figsize=(18, 10)) -> plt.Figure:\n",
    "        st = self.stats()\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        gs = GridSpec(2, 3, figure=fig, height_ratios=[1, 1.2])\n",
    "\n",
    "        def _annot_bar(ax, data, color, ttl):\n",
    "            if not data: # Check if data dictionary is empty\n",
    "                ax.text(0.5, 0.5, \"No data to display\", ha=\"center\", va=\"center\")\n",
    "                ax.set_axis_off()\n",
    "                return\n",
    "            \n",
    "            labels, vals = zip(*sorted(data.items())) # Sort for consistent bar order\n",
    "            \n",
    "            # Ensure vals is suitable for sum; handle potential non-numeric if data was structured unexpectedly\n",
    "            valid_vals = [v for v in vals if isinstance(v, (int, float))]\n",
    "            if not valid_vals or sum(valid_vals) == 0: # also check if sum is zero\n",
    "                perc = np.zeros_like(labels, dtype=float)\n",
    "            else:\n",
    "                perc = 100 * np.array(vals) / sum(valid_vals) # Normalize by sum of counts in this category\n",
    "\n",
    "            bars = ax.bar(range(len(labels)), perc, color=color) # Use range for x, then set_xticklabels\n",
    "            ax.set_xticks(range(len(labels)))\n",
    "            ax.set_xticklabels([str(l) for l in labels], rotation=45, ha=\"right\", fontsize=8)\n",
    "\n",
    "\n",
    "            for bar_obj, p_val in zip(bars, perc): # Renamed bar to bar_obj\n",
    "                if p_val > 0.5: # Threshold for displaying text\n",
    "                    ax.text(bar_obj.get_x() + bar_obj.get_width() / 2,\n",
    "                            p_val + 0.3, # Position text above bar\n",
    "                            f\"{p_val:.1f}%\",\n",
    "                            ha=\"center\", va=\"bottom\", fontsize=7)\n",
    "            ax.set_title(ttl)\n",
    "            ax.set_ylabel(\"% frames within this category\") # Clarified ylabel\n",
    "            ax.tick_params(axis=\"x\", rotation=45, labelsize=8)\n",
    "            ax.set_ylim(0, max(10, np.max(perc) * 1.1 if len(perc)>0 and np.max(perc) > 0 else 10)) # Dynamic Y limit\n",
    "\n",
    "        # Summary box\n",
    "        ax0 = fig.add_subplot(gs[0, 0]); ax0.axis(\"off\")\n",
    "        txt = (\n",
    "            f\"Overall FER : {self.frame_error_rate():.1f}%\\n\"\n",
    "            f\"Macro FER   : {self.macro_fer():.1f}%\\n\"\n",
    "            f\"V‑measure   : {self.v_measure():.3f}\\n\\n\"\n",
    "            f\"GT types    : {len(self.gt_types)}\\n\"\n",
    "            f\"Pred types  : {len(self.pred_types)}\\n\"\n",
    "            f\"Mapped GT types: {st['pct_types_mapped']:.1f}%\"\n",
    "        )\n",
    "        ax0.text(0.05, 0.95, txt, va=\"top\", ha=\"left\", fontsize=10, bbox=dict(fc=\"whitesmoke\", alpha=.8, boxstyle=\"round,pad=0.5\"))\n",
    "\n",
    "        # Pies for types and frames mapped\n",
    "        ax1 = fig.add_subplot(gs[0, 1])\n",
    "        if self.gt_types.size > 0 :\n",
    "             ax1.pie([st['pct_types_mapped'], 100 - st['pct_types_mapped']], labels=[\"Mapped\", \"Unmapped\"],\n",
    "                     autopct=\"%.1f%%\", colors=[\"#8fd175\", \"#f28e8e\"], startangle=90)\n",
    "        else:\n",
    "            ax1.text(0.5, 0.5, \"No GT types\", ha=\"center\", va=\"center\")\n",
    "        ax1.set_title(\"GT Label Types\")\n",
    "\n",
    "        ax2 = fig.add_subplot(gs[0, 2])\n",
    "        if self.gt.size > 0:\n",
    "            ax2.pie([st['pct_frames_mapped'], 100 - st['pct_frames_mapped']], labels=[\"Mapped\", \"Unmapped\"],\n",
    "                    autopct=\"%.1f%%\", colors=[\"#71b3ff\", \"#ffb471\"], startangle=90)\n",
    "        else:\n",
    "            ax2.text(0.5, 0.5, \"No GT frames\", ha=\"center\", va=\"center\")\n",
    "        ax2.set_title(\"GT Frames\")\n",
    "        \n",
    "        # Bars for mapped and unmapped counts\n",
    "        ax3 = fig.add_subplot(gs[1, 0:2]) # Spans two columns\n",
    "        _annot_bar(ax3, st['mapped_counts'], \"#4daf4a\", \"Mapped GT Labels (% of Mapped Frames)\")\n",
    "        \n",
    "        ax4 = fig.add_subplot(gs[1, 2]) # Single column\n",
    "        _annot_bar(ax4, st['unmapped_counts'], \"#d73027\", \"Unmapped GT Labels (% of Unmapped Frames)\")\n",
    "\n",
    "        fig.suptitle(title, fontsize=16, y=0.98) # Adjust y for suptitle\n",
    "        fig.tight_layout(rect=[0, 0, 1, 0.95]) # Adjust rect to prevent suptitle overlap\n",
    "        return fig\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Smoothing Helper Functions\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def basic_majority_vote(labels: np.ndarray, window_size: int) -> np.ndarray:\n",
    "    \"\"\"Apply basic majority vote smoothing to a 1D array of labels.\"\"\"\n",
    "    if window_size <= 1 or len(labels) == 0:\n",
    "        return labels.copy()\n",
    "    \n",
    "    n = len(labels)\n",
    "    smoothed_labels = np.copy(labels)\n",
    "    half_window = window_size // 2\n",
    "    \n",
    "    for i in range(n):\n",
    "        start = max(0, i - half_window)\n",
    "        end = min(n, i + half_window + 1)\n",
    "        window = labels[start:end]\n",
    "        \n",
    "        if len(window) > 0:\n",
    "            counts = Counter(window)\n",
    "            # Simple tie-breaking: pick the first one encountered (Python's default for Counter.most_common)\n",
    "            # or could be counts.most_common(1)[0][0]\n",
    "            # To make it slightly more stable or prefer original if it's a tie:\n",
    "            top_two = counts.most_common(2)\n",
    "            if len(top_two) == 1: # Only one item in window or all same\n",
    "                 smoothed_labels[i] = top_two[0][0]\n",
    "            elif top_two[0][1] > top_two[1][1]: # Clear winner\n",
    "                 smoothed_labels[i] = top_two[0][0]\n",
    "            else: # Tie, prefer original label if it's among the most common\n",
    "                tied_labels = [item[0] for item in top_two if item[1] == top_two[0][1]]\n",
    "                if labels[i] in tied_labels:\n",
    "                    smoothed_labels[i] = labels[i]\n",
    "                else:\n",
    "                    smoothed_labels[i] = top_two[0][0] # Fallback to the first most common\n",
    "        # If window is empty (should not happen with proper start/end), original label is kept\n",
    "            \n",
    "    return smoothed_labels\n",
    "\n",
    "def smooth_labels_per_sequence(\n",
    "    raw_labels: np.ndarray, \n",
    "    dataset_indices: np.ndarray, \n",
    "    window_size: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Apply majority vote smoothing independently to each sequence defined by dataset_indices.\"\"\"\n",
    "    if window_size <= 1:\n",
    "        return raw_labels.copy()\n",
    "    \n",
    "    smoothed_labels = np.zeros_like(raw_labels)\n",
    "    unique_indices = np.unique(dataset_indices)\n",
    "    \n",
    "    for seq_idx in unique_indices:\n",
    "        mask = (dataset_indices == seq_idx)\n",
    "        sequence_labels = raw_labels[mask]\n",
    "        if len(sequence_labels) > 0: # Ensure there are labels for this sequence\n",
    "            smoothed_labels[mask] = basic_majority_vote(sequence_labels, window_size)\n",
    "        # If sequence_labels is empty, corresponding part of smoothed_labels remains 0 or its initial value.\n",
    "            \n",
    "    return smoothed_labels\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Main Evaluation Function (Simplified)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def evaluate_predictions_with_smoothing(\n",
    "    gt_labels: np.ndarray,              # Ground truth labels\n",
    "    predicted_labels_raw: np.ndarray, # Raw predicted labels (e.g., from decoder)\n",
    "    dataset_indices: np.ndarray,      # Array indicating sequence/song boundaries for smoothing\n",
    "    output_dir: str,                  # Directory to save reports\n",
    "    smoothing_windows: List[int] = [0, 200], # Window sizes for smoothing\n",
    "    base_title: str = \"Decoder\"       # Base for plot titles\n",
    ") -> Dict[int, ClusteringMetrics]:\n",
    "    \"\"\"\n",
    "    Evaluates predicted labels against ground truth for specified smoothing windows.\n",
    "    Generates a report (plot and text summary) for each window.\n",
    "    \"\"\"\n",
    "    output_path = pathlib.Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    metrics_results_per_window: Dict[int, ClusteringMetrics] = {}\n",
    "\n",
    "    print(f\"Starting evaluation for base title: {base_title}\")\n",
    "    print(f\"Ground truth labels shape: {gt_labels.shape}\")\n",
    "    print(f\"Raw predicted labels shape: {predicted_labels_raw.shape}\")\n",
    "    print(f\"Dataset indices shape: {dataset_indices.shape}\")\n",
    "\n",
    "    for window_size in smoothing_windows:\n",
    "        print(f\"\\n--- Evaluating with smoothing window: {window_size} ---\")\n",
    "        \n",
    "        # Apply smoothing\n",
    "        if window_size == 0:\n",
    "            smoothed_predictions = predicted_labels_raw.copy()\n",
    "            print(\"Using raw predictions (no smoothing).\")\n",
    "        else:\n",
    "            if dataset_indices.size == 0:\n",
    "                print(\"Warning: dataset_indices is empty. Applying smoothing globally instead of per-sequence.\")\n",
    "                smoothed_predictions = basic_majority_vote(predicted_labels_raw, window_size)\n",
    "            elif len(np.unique(dataset_indices)) == 1 and len(dataset_indices) == len(predicted_labels_raw):\n",
    "                print(f\"Applying smoothing with window {window_size} to the whole sequence (single dataset index).\")\n",
    "                smoothed_predictions = basic_majority_vote(predicted_labels_raw, window_size)\n",
    "            else:\n",
    "                print(f\"Applying per-sequence smoothing with window {window_size}.\")\n",
    "                smoothed_predictions = smooth_labels_per_sequence(predicted_labels_raw, dataset_indices, window_size)\n",
    "        print(f\"Shape of smoothed predictions: {smoothed_predictions.shape}\")\n",
    "\n",
    "        # Calculate metrics using the ClusteringMetrics class\n",
    "        cm = ClusteringMetrics(gt_labels, smoothed_predictions)\n",
    "        metrics_results_per_window[window_size] = cm\n",
    "        \n",
    "        # --- Generate and Save Report for this window ---\n",
    "        window_report_dir = output_path / f\"{base_title.replace(' ', '_')}_window_{window_size}\"\n",
    "        window_report_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Dashboard Plot\n",
    "        plot_title = f\"{base_title} Evaluation (Smoothing Window: {window_size})\"\n",
    "        fig = cm.plot(title=plot_title)\n",
    "        plot_save_path = window_report_dir / \"metrics_dashboard.png\"\n",
    "        fig.savefig(plot_save_path, dpi=150, bbox_inches=\"tight\")\n",
    "        plt.close(fig) # Close plot to free memory\n",
    "        \n",
    "        # Text Summary\n",
    "        summary_text_path = window_report_dir / \"summary_metrics.txt\"\n",
    "        stats_data = cm.stats() # from ClusteringMetrics\n",
    "        with open(summary_text_path, \"w\") as f:\n",
    "            f.write(f\"Metrics Summary for: {base_title}\\n\")\n",
    "            f.write(f\"Smoothing Window Size: {window_size}\\n\")\n",
    "            f.write(\"-------------------------------------------------\\n\")\n",
    "            f.write(f\"V-measure Score          : {cm.v_measure():.4f}\\n\")\n",
    "            f.write(f\"Total FER               : {cm.total_fer():.2f}%\\n\")\n",
    "            f.write(f\"Matched-only FER        : {cm.matched_fer():.2f}%\\n\")\n",
    "            f.write(f\"Macro Frame Error Rate   : {cm.macro_fer():.2f}%\\n\")\n",
    "            f.write(\"-------------------------------------------------\\n\")\n",
    "            f.write(f\"GT Label Types           : {len(cm.gt_types)}\\n\")\n",
    "            f.write(f\"Predicted Label Types    : {len(cm.pred_types)}\\n\")\n",
    "            f.write(f\"% GT Types Mapped        : {stats_data['pct_types_mapped']:.2f}%\\n\")\n",
    "            f.write(f\"% GT Frames Mapped       : {stats_data['pct_frames_mapped']:.2f}%\\n\")\n",
    "            f.write(\"-------------------------------------------------\\n\")\n",
    "            f.write(\"Mapped GT Label Counts:\\n\")\n",
    "            for label, count in sorted(stats_data['mapped_counts'].items()):\n",
    "                f.write(f\"  Label {label}: {count}\\n\")\n",
    "            f.write(\"\\nUnmapped GT Label Counts:\\n\")\n",
    "            for label, count in sorted(stats_data['unmapped_counts'].items()):\n",
    "                f.write(f\"  Label {label}: {count}\\n\")\n",
    "        \n",
    "        print(f\"Saved reports for window {window_size} to: {window_report_dir}\")\n",
    "        print(f\"  V-measure: {cm.v_measure():.3f}, FER: {cm.frame_error_rate():.1f}%, Macro FER: {cm.macro_fer():.1f}%\")\n",
    "\n",
    "    # Dump all metrics to a txt file at the end\n",
    "    all_metrics_txt = output_path / \"all_metrics_dump.txt\"\n",
    "    with open(all_metrics_txt, \"w\") as f:\n",
    "        for window_size, cm in metrics_results_per_window.items():\n",
    "            stats_data = cm.stats()\n",
    "            f.write(f\"==== Window Size: {window_size} ====\\n\")\n",
    "            f.write(f\"V-measure Score          : {cm.v_measure():.4f}\\n\")\n",
    "            f.write(f\"Total FER               : {cm.total_fer():.2f}%\\n\")\n",
    "            f.write(f\"Matched-only FER        : {cm.matched_fer():.2f}%\\n\")\n",
    "            f.write(f\"Macro Frame Error Rate   : {cm.macro_fer():.2f}%\\n\")\n",
    "            f.write(f\"GT Label Types           : {len(cm.gt_types)}\\n\")\n",
    "            f.write(f\"Predicted Label Types    : {len(cm.pred_types)}\\n\")\n",
    "            f.write(f\"% GT Types Mapped        : {stats_data['pct_types_mapped']:.2f}%\\n\")\n",
    "            f.write(f\"% GT Frames Mapped       : {stats_data['pct_frames_mapped']:.2f}%\\n\")\n",
    "            f.write(\"Mapped GT Label Counts:\\n\")\n",
    "            for label, count in sorted(stats_data['mapped_counts'].items()):\n",
    "                f.write(f\"  Label {label}: {count}\\n\")\n",
    "            f.write(\"Unmapped GT Label Counts:\\n\")\n",
    "            for label, count in sorted(stats_data['unmapped_counts'].items()):\n",
    "                f.write(f\"  Label {label}: {count}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "    print(f\"Dumped all metrics to {all_metrics_txt}\")\n",
    "\n",
    "    return metrics_results_per_window\n",
    "\n",
    "import pathlib\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # iterate over all .npz result files and just print metrics\n",
    "    results_dir = pathlib.Path(\"/home/george-vengrovski/Documents/projects/tweety_bert_paper/results/decoder_avg_perf_eval\")\n",
    "    smoothing_windows = [0, 50, 100]\n",
    "\n",
    "    all_metrics = []\n",
    "    for results_file in results_dir.iterdir():\n",
    "        if results_file.suffix != \".npz\":\n",
    "            continue\n",
    "        print(f\"\\nprocessing {results_file.name}\")\n",
    "        try:\n",
    "            data = np.load(results_file)\n",
    "            gt = data.get(\"ground_truth_labels\")\n",
    "            pred = data.get(\"decoder_labels\")\n",
    "            if gt is None or pred is None:\n",
    "                print(\"  missing ground_truth_labels or decoder_labels, skipping\")\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(f\"  failed to load {results_file.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        for w in smoothing_windows:\n",
    "            if w == 0:\n",
    "                sm = pred.copy()\n",
    "            else:\n",
    "                sm = basic_majority_vote(pred, w)\n",
    "            cm = ClusteringMetrics(gt, sm)\n",
    "            print(\n",
    "                f\"  window {w}: \"\n",
    "                f\"v-measure={cm.v_measure():.3f}, \"\n",
    "                f\"total_fer={cm.total_fer():.2f}%, \"\n",
    "                f\"matched_fer={cm.matched_fer():.2f}%, \"\n",
    "                f\"macro_fer={cm.macro_fer():.2f}%\"\n",
    "            )\n",
    "            all_metrics.append({\n",
    "                \"file\": results_file.name,\n",
    "                \"window\": w,\n",
    "                \"v_measure\": cm.v_measure(),\n",
    "                \"total_fer\": cm.total_fer(),\n",
    "                \"matched_fer\": cm.matched_fer(),\n",
    "                \"macro_fer\": cm.macro_fer()\n",
    "            })\n",
    "\n",
    "    # Dump all metrics to a txt file\n",
    "    metrics_txt_path = results_dir / \"all_metrics_dump.txt\"\n",
    "    with open(metrics_txt_path, \"w\") as f:\n",
    "        for entry in all_metrics:\n",
    "            f.write(\n",
    "                f\"File: {entry['file']}, Window: {entry['window']}\\n\"\n",
    "                f\"  v-measure={entry['v_measure']:.3f}, \"\n",
    "                f\"total_fer={entry['total_fer']:.2f}%, \"\n",
    "                f\"matched_fer={entry['matched_fer']:.2f}%, \"\n",
    "                f\"macro_fer={entry['macro_fer']:.2f}%\\n\"\n",
    "            )\n",
    "    print(f\"Dumped all metrics to {metrics_txt_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweetybert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
