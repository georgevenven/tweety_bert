{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'num_freq_bins'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m weights_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/home/george-vengrovski/Documents/projects/tweety_bert_paper/experiments/TweetyBERT-MSE-Mask-Before-50-mask-alpha-1/saved_weights/model_step_6400.pth\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m config_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/home/george-vengrovski/Documents/projects/tweety_bert_paper/experiments/TweetyBERT-MSE-Mask-Before-50-mask-alpha-1/config.json\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m tweety_bert_model \u001b[39m=\u001b[39m load_model(config_path, weights_path)\n\u001b[1;32m     20\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/projects/tweety_bert_paper/src/utils.py:72\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(config_path, weight_path)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mInitialize and load the model with the given configuration and weights.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mtorch.nn.Module: The initialized model.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m config \u001b[39m=\u001b[39m load_config(config_path)\n\u001b[1;32m     69\u001b[0m model \u001b[39m=\u001b[39m TweetyBERT(\n\u001b[1;32m     70\u001b[0m     d_transformer\u001b[39m=\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39md_transformer\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[1;32m     71\u001b[0m     nhead_transformer\u001b[39m=\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mnhead_transformer\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m---> 72\u001b[0m     num_freq_bins\u001b[39m=\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mnum_freq_bins\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     73\u001b[0m     num_labels\u001b[39m=\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mnum_ground_truth_labels\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     74\u001b[0m     dropout\u001b[39m=\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mdropout\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     75\u001b[0m     dim_feedforward\u001b[39m=\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mdim_feedforward\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     76\u001b[0m     transformer_layers\u001b[39m=\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mtransformer_layers\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     77\u001b[0m     m\u001b[39m=\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mm\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     78\u001b[0m     p\u001b[39m=\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mp\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     79\u001b[0m     alpha\u001b[39m=\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39malpha\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     80\u001b[0m     pos_enc_type\u001b[39m=\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mpos_enc_type\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     81\u001b[0m     length\u001b[39m=\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     82\u001b[0m )\n\u001b[1;32m     84\u001b[0m \u001b[39mif\u001b[39;00m weight_path:\n\u001b[1;32m     85\u001b[0m     load_weights(\u001b[39mdir\u001b[39m\u001b[39m=\u001b[39mweight_path, model\u001b[39m=\u001b[39mmodel)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'num_freq_bins'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "sys.path.append(\"src\")\n",
    "os.chdir('/home/george-vengrovski/Documents/projects/tweety_bert_paper')\n",
    "\n",
    "from data_class import CollateFunction\n",
    "from utils import load_model\n",
    "\n",
    "weights_path = \"/home/george-vengrovski/Documents/projects/tweety_bert_paper/experiments/TweetyBERT-MSE-Mask-Before-50-mask-alpha-1/saved_weights/model_step_6400.pth\"\n",
    "config_path = \"/home/george-vengrovski/Documents/projects/tweety_bert_paper/experiments/TweetyBERT-MSE-Mask-Before-50-mask-alpha-1/config.json\"\n",
    "\n",
    "tweety_bert_model = load_model(config_path, weights_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from data_class import SongDataSet_Image\n",
    "\n",
    "train_dir = \"/home/george-vengrovski/Documents/projects/tweety_bert_paper/files/llb3_train\"\n",
    "test_dir = \"/home/george-vengrovski/Documents/projects/tweety_bert_paper/files/llb3_test\"\n",
    "\n",
    "train_dataset = SongDataSet_Image(train_dir, num_classes=196)\n",
    "test_dataset = SongDataSet_Image(test_dir, num_classes=196)\n",
    "\n",
    "collate_fn = CollateFunction(segment_length=1000)  # Adjust the segment length if needed\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=48, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=48, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Linear Classifier and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_probe import LinearProbeModel, LinearProbeTrainer\n",
    "\n",
    "classifier_model = LinearProbeModel(num_classes=21, model_type=\"neural_net\", model=tweety_bert_model, freeze_layers=False, layer_num=2, layer_id=\"attention_output\", classifier_dims=196)\n",
    "classifier_model = classifier_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10: FER = 49.30%, Train Loss = 6.2491, Val Loss = 4.9879\n",
      "Batch 20: FER = 23.11%, Train Loss = 3.0641, Val Loss = 2.6108\n",
      "Batch 30: FER = 14.07%, Train Loss = 1.7632, Val Loss = 1.4944\n",
      "Batch 40: FER = 8.23%, Train Loss = 0.9761, Val Loss = 0.8982\n",
      "Batch 50: FER = 7.65%, Train Loss = 0.4146, Val Loss = 0.4678\n",
      "Batch 60: FER = 6.46%, Train Loss = 0.3548, Val Loss = 0.3687\n",
      "Batch 70: FER = 5.91%, Train Loss = 0.4697, Val Loss = 0.2648\n",
      "Batch 80: FER = 5.88%, Train Loss = 0.2781, Val Loss = 0.2467\n",
      "Batch 90: FER = 5.89%, Train Loss = 0.2046, Val Loss = 0.2413\n",
      "Batch 100: FER = 4.98%, Train Loss = 0.1679, Val Loss = 0.1865\n",
      "Batch 110: FER = 4.58%, Train Loss = 0.2109, Val Loss = 0.1724\n",
      "Batch 120: FER = 4.88%, Train Loss = 0.1643, Val Loss = 0.1813\n",
      "Batch 130: FER = 5.81%, Train Loss = 0.1621, Val Loss = 0.2253\n",
      "Batch 140: FER = 5.74%, Train Loss = 0.1445, Val Loss = 0.2085\n",
      "Batch 150: FER = 4.84%, Train Loss = 0.1713, Val Loss = 0.1774\n",
      "Batch 160: FER = 4.43%, Train Loss = 0.1671, Val Loss = 0.1566\n",
      "Batch 170: FER = 4.13%, Train Loss = 0.1268, Val Loss = 0.1448\n",
      "Batch 180: FER = 4.03%, Train Loss = 0.1290, Val Loss = 0.1313\n",
      "Batch 190: FER = 3.88%, Train Loss = 0.1358, Val Loss = 0.1258\n",
      "Batch 200: FER = 4.16%, Train Loss = 0.1370, Val Loss = 0.1391\n",
      "Batch 210: FER = 3.72%, Train Loss = 0.1167, Val Loss = 0.1167\n",
      "Batch 220: FER = 4.39%, Train Loss = 0.1427, Val Loss = 0.1381\n",
      "Batch 230: FER = 4.07%, Train Loss = 0.1164, Val Loss = 0.1311\n",
      "Batch 240: FER = 4.14%, Train Loss = 0.1530, Val Loss = 0.1306\n",
      "Batch 250: FER = 4.24%, Train Loss = 0.1205, Val Loss = 0.1356\n",
      "Batch 260: FER = 3.97%, Train Loss = 0.1245, Val Loss = 0.1328\n",
      "Batch 270: FER = 3.75%, Train Loss = 0.1203, Val Loss = 0.1234\n",
      "Batch 280: FER = 4.16%, Train Loss = 0.1274, Val Loss = 0.1363\n",
      "Batch 290: FER = 3.99%, Train Loss = 0.1541, Val Loss = 0.1234\n",
      "Batch 300: FER = 3.68%, Train Loss = 0.1153, Val Loss = 0.1163\n",
      "Batch 310: FER = 3.94%, Train Loss = 0.1136, Val Loss = 0.1271\n",
      "Batch 320: FER = 3.76%, Train Loss = 0.1182, Val Loss = 0.1207\n",
      "Batch 330: FER = 3.91%, Train Loss = 0.1739, Val Loss = 0.1263\n",
      "Batch 340: FER = 3.65%, Train Loss = 0.1347, Val Loss = 0.1144\n",
      "Batch 350: FER = 3.67%, Train Loss = 0.1109, Val Loss = 0.1173\n",
      "Batch 360: FER = 3.94%, Train Loss = 0.1230, Val Loss = 0.1186\n",
      "Batch 370: FER = 3.79%, Train Loss = 0.0808, Val Loss = 0.1169\n",
      "Batch 380: FER = 3.62%, Train Loss = 0.1274, Val Loss = 0.1119\n",
      "Batch 390: FER = 3.69%, Train Loss = 0.1063, Val Loss = 0.1117\n",
      "Batch 400: FER = 3.83%, Train Loss = 0.1160, Val Loss = 0.1192\n",
      "Batch 410: FER = 3.82%, Train Loss = 0.1090, Val Loss = 0.1180\n",
      "Batch 420: FER = 3.58%, Train Loss = 0.0948, Val Loss = 0.1123\n",
      "Batch 430: FER = 3.86%, Train Loss = 0.1046, Val Loss = 0.1204\n",
      "Batch 440: FER = 4.03%, Train Loss = 0.1218, Val Loss = 0.1237\n",
      "Batch 450: FER = 3.65%, Train Loss = 0.1182, Val Loss = 0.1105\n",
      "Batch 460: FER = 3.54%, Train Loss = 0.1204, Val Loss = 0.1076\n",
      "Batch 470: FER = 3.45%, Train Loss = 0.1054, Val Loss = 0.1058\n",
      "Batch 480: FER = 3.63%, Train Loss = 0.1153, Val Loss = 0.1114\n",
      "Batch 490: FER = 3.55%, Train Loss = 0.0898, Val Loss = 0.1073\n",
      "Batch 500: FER = 3.45%, Train Loss = 0.1243, Val Loss = 0.1047\n",
      "Batch 510: FER = 3.76%, Train Loss = 0.1088, Val Loss = 0.1114\n",
      "Batch 520: FER = 3.64%, Train Loss = 0.0991, Val Loss = 0.1094\n",
      "Batch 530: FER = 3.50%, Train Loss = 0.0864, Val Loss = 0.1041\n",
      "Batch 540: FER = 3.41%, Train Loss = 0.0978, Val Loss = 0.1005\n",
      "Batch 550: FER = 3.43%, Train Loss = 0.1028, Val Loss = 0.1050\n",
      "Batch 560: FER = 3.43%, Train Loss = 0.1272, Val Loss = 0.0984\n",
      "Batch 570: FER = 3.48%, Train Loss = 0.1116, Val Loss = 0.1036\n",
      "Batch 580: FER = 3.54%, Train Loss = 0.0801, Val Loss = 0.1082\n",
      "Batch 590: FER = 3.47%, Train Loss = 0.1082, Val Loss = 0.1001\n",
      "Batch 600: FER = 3.37%, Train Loss = 0.0982, Val Loss = 0.1017\n",
      "Batch 610: FER = 3.39%, Train Loss = 0.0955, Val Loss = 0.1038\n",
      "Batch 620: FER = 3.72%, Train Loss = 0.1205, Val Loss = 0.1152\n",
      "Batch 630: FER = 3.62%, Train Loss = 0.1118, Val Loss = 0.1101\n",
      "Batch 640: FER = 3.13%, Train Loss = 0.0868, Val Loss = 0.0901\n",
      "Batch 650: FER = 3.37%, Train Loss = 0.1106, Val Loss = 0.1025\n",
      "Batch 660: FER = 3.41%, Train Loss = 0.1025, Val Loss = 0.1027\n",
      "Batch 670: FER = 3.51%, Train Loss = 0.0865, Val Loss = 0.1085\n",
      "Batch 680: FER = 3.60%, Train Loss = 0.0685, Val Loss = 0.1049\n",
      "Batch 690: FER = 3.33%, Train Loss = 0.0954, Val Loss = 0.0996\n",
      "Batch 700: FER = 3.28%, Train Loss = 0.0774, Val Loss = 0.0931\n",
      "Batch 710: FER = 3.43%, Train Loss = 0.0839, Val Loss = 0.1010\n",
      "Batch 720: FER = 3.31%, Train Loss = 0.1092, Val Loss = 0.0981\n",
      "Batch 730: FER = 3.49%, Train Loss = 0.0832, Val Loss = 0.1023\n",
      "Batch 740: FER = 3.28%, Train Loss = 0.0919, Val Loss = 0.0963\n",
      "Batch 750: FER = 3.21%, Train Loss = 0.1153, Val Loss = 0.0952\n",
      "Batch 760: FER = 3.14%, Train Loss = 0.0762, Val Loss = 0.0938\n",
      "Batch 770: FER = 3.38%, Train Loss = 0.0881, Val Loss = 0.0995\n",
      "Batch 780: FER = 3.21%, Train Loss = 0.0921, Val Loss = 0.0929\n",
      "Batch 790: FER = 3.21%, Train Loss = 0.1310, Val Loss = 0.0939\n",
      "Batch 800: FER = 3.27%, Train Loss = 0.1021, Val Loss = 0.0949\n",
      "Batch 810: FER = 3.35%, Train Loss = 0.0902, Val Loss = 0.0954\n",
      "Batch 820: FER = 3.42%, Train Loss = 0.0910, Val Loss = 0.1014\n",
      "Batch 830: FER = 3.19%, Train Loss = 0.0867, Val Loss = 0.0920\n",
      "Batch 840: FER = 3.28%, Train Loss = 0.0879, Val Loss = 0.0975\n",
      "Batch 850: FER = 3.45%, Train Loss = 0.0850, Val Loss = 0.0974\n",
      "Batch 860: FER = 3.34%, Train Loss = 0.0920, Val Loss = 0.0943\n",
      "Batch 870: FER = 3.28%, Train Loss = 0.0836, Val Loss = 0.0944\n",
      "Batch 880: FER = 3.23%, Train Loss = 0.1070, Val Loss = 0.0947\n",
      "Batch 890: FER = 3.16%, Train Loss = 0.1082, Val Loss = 0.0904\n",
      "Batch 900: FER = 3.25%, Train Loss = 0.0868, Val Loss = 0.0936\n",
      "Batch 910: FER = 3.24%, Train Loss = 0.0951, Val Loss = 0.0922\n",
      "Batch 920: FER = 3.42%, Train Loss = 0.0963, Val Loss = 0.0985\n",
      "Batch 930: FER = 3.31%, Train Loss = 0.0894, Val Loss = 0.0955\n",
      "Batch 940: FER = 3.22%, Train Loss = 0.0770, Val Loss = 0.0913\n",
      "Batch 950: FER = 3.16%, Train Loss = 0.0988, Val Loss = 0.0902\n",
      "Batch 960: FER = 3.21%, Train Loss = 0.0847, Val Loss = 0.0912\n",
      "Batch 970: FER = 3.25%, Train Loss = 0.0992, Val Loss = 0.0962\n",
      "Batch 980: FER = 3.24%, Train Loss = 0.0822, Val Loss = 0.0935\n",
      "Batch 990: FER = 3.31%, Train Loss = 0.1000, Val Loss = 0.0955\n",
      "Batch 1000: FER = 3.24%, Train Loss = 0.0765, Val Loss = 0.0941\n",
      "Batch 1010: FER = 3.36%, Train Loss = 0.0990, Val Loss = 0.0965\n",
      "Batch 1020: FER = 3.26%, Train Loss = 0.0802, Val Loss = 0.0932\n",
      "Batch 1030: FER = 3.20%, Train Loss = 0.0850, Val Loss = 0.0924\n",
      "Batch 1040: FER = 3.32%, Train Loss = 0.0988, Val Loss = 0.0957\n",
      "Batch 1050: FER = 3.17%, Train Loss = 0.0938, Val Loss = 0.0927\n",
      "Batch 1060: FER = 3.24%, Train Loss = 0.0818, Val Loss = 0.0929\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[39m=\u001b[39m LinearProbeTrainer(model\u001b[39m=\u001b[39mclassifier_model, train_loader\u001b[39m=\u001b[39mtrain_loader, test_loader\u001b[39m=\u001b[39mtest_loader, device\u001b[39m=\u001b[39mdevice, lr\u001b[39m=\u001b[39m\u001b[39m3e-4\u001b[39m, plotting\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, batches_per_eval\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, desired_total_batches\u001b[39m=\u001b[39m\u001b[39m1e4\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/Documents/projects/tweety_bert_paper/src/linear_probe.py:169\u001b[0m, in \u001b[0;36mLinearProbeTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m total_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    168\u001b[0m \u001b[39mif\u001b[39;00m total_batches \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatches_per_eval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 169\u001b[0m     avg_val_loss, avg_frame_error \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidate_model()\n\u001b[1;32m    171\u001b[0m     raw_loss_list\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[1;32m    172\u001b[0m     raw_val_loss_list\u001b[39m.\u001b[39mappend(avg_val_loss)\n",
      "File \u001b[0;32m~/Documents/projects/tweety_bert_paper/src/linear_probe.py:131\u001b[0m, in \u001b[0;36mLinearProbeTrainer.validate_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[1;32m    130\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mcross_entropy_loss(predictions\u001b[39m=\u001b[39moutput, targets\u001b[39m=\u001b[39mlabel)\n\u001b[0;32m--> 131\u001b[0m total_val_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m    132\u001b[0m total_frame_error \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframe_error_rate(output, label)\u001b[39m.\u001b[39mitem()\n\u001b[1;32m    133\u001b[0m num_val_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = LinearProbeTrainer(model=classifier_model, train_loader=train_loader, test_loader=test_loader, device=device, lr=3e-4, plotting=True, batches_per_eval=10, desired_total_batches=100, patience=4)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'linear_probe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlinear_probe\u001b[39;00m \u001b[39mimport\u001b[39;00m ModelEvaluator\n\u001b[1;32m      4\u001b[0m \u001b[39m# Initialize the ModelEvaluator with the filter_unseen_classes feature\u001b[39;00m\n\u001b[1;32m      5\u001b[0m evaluator \u001b[39m=\u001b[39m ModelEvaluator(model\u001b[39m=\u001b[39mclassifier_model, \n\u001b[1;32m      6\u001b[0m                            test_loader\u001b[39m=\u001b[39mtest_loader, \n\u001b[1;32m      7\u001b[0m                            num_classes\u001b[39m=\u001b[39m\u001b[39m21\u001b[39m,  \u001b[39m# Assuming there are 21 possible classes\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m                            filter_unseen_classes\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,  \u001b[39m# Enable filtering based on training set classes\u001b[39;00m\n\u001b[1;32m     11\u001b[0m                            train_dir\u001b[39m=\u001b[39mtrain_dir)  \u001b[39m# Path to the training dataset directory\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'linear_probe'"
     ]
    }
   ],
   "source": [
    "from linear_probe import ModelEvaluator\n",
    "\n",
    "\n",
    "# Initialize the ModelEvaluator with the filter_unseen_classes feature\n",
    "evaluator = ModelEvaluator(model=classifier_model, \n",
    "                           test_loader=test_loader, \n",
    "                           num_classes=21,  # Assuming there are 21 possible classes\n",
    "                           device='cuda:0',  # Use CUDA if available\n",
    "                           use_tqdm=True,  # Enable a progress bar for the evaluation\n",
    "                           filter_unseen_classes=True,  # Enable filtering based on training set classes\n",
    "                           train_dir=train_dir)  # Path to the training dataset directory\n",
    "\n",
    "# Perform model validation with multiple passes\n",
    "class_frame_error_rates, total_frame_error_rate = evaluator.validate_model_multiple_passes(num_passes=1, max_batches=1250)\n",
    "\n",
    "# Save the results to a specified directory, for example 'evaluation_results'\n",
    "evaluator.save_results(class_frame_error_rates, total_frame_error_rate, '')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "canary-vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
