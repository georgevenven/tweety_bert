{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch \n",
    "import json\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"src\")\n",
    "os.chdir('/home/george-vengrovski/Documents/projects/tweety_bert_paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from data_class import SongDataSet_Image, CollateFunction\n",
    "\n",
    "train_dir = \"/home/george-vengrovski/Documents/projects/tweety_bert_paper/files/llb3_train_1_precent\"\n",
    "test_dir = \"/home/george-vengrovski/Documents/projects/tweety_bert_paper/files/llb3_test\"\n",
    "\n",
    "train_dataset = SongDataSet_Image(train_dir, num_classes=21)\n",
    "test_dataset = SongDataSet_Image(test_dir, num_classes=21)\n",
    "\n",
    "collate_fn = CollateFunction(segment_length=1000)  # Adjust the segment length if needed\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_probe import LinearProbeModel, LinearProbeTrainer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "classifier_model = LinearProbeModel(num_classes=21, model_type=\"raw\", model=None, freeze_layers=None, layer_num=None, layer_id=None, classifier_dims=196)\n",
    "classifier_model = classifier_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100: FER = 57.68%, Train Loss = 2.1114, Val Loss = 2.3344\n",
      "Batch 200: FER = 54.12%, Train Loss = 1.8949, Val Loss = 2.0793\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[39m=\u001b[39m LinearProbeTrainer(model\u001b[39m=\u001b[39mclassifier_model, train_loader\u001b[39m=\u001b[39mtrain_loader, test_loader\u001b[39m=\u001b[39mtest_loader, device\u001b[39m=\u001b[39mdevice, lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m, plotting\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, batches_per_eval\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, desired_total_batches\u001b[39m=\u001b[39m\u001b[39m1e4\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/Documents/projects/tweety_bert_paper/src/linear_probe.py:154\u001b[0m, in \u001b[0;36mLinearProbeTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    151\u001b[0m raw_loss_list, raw_val_loss_list, raw_frame_error_rate_list \u001b[39m=\u001b[39m [], [], []\n\u001b[1;32m    153\u001b[0m \u001b[39mwhile\u001b[39;00m total_batches \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdesired_total_batches:\n\u001b[0;32m--> 154\u001b[0m     \u001b[39mfor\u001b[39;00m i, (spectrogram, label) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_loader):\n\u001b[1;32m    155\u001b[0m         \u001b[39mif\u001b[39;00m total_batches \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdesired_total_batches:\n\u001b[1;32m    156\u001b[0m             \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tweetybert/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/tweetybert/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/tweetybert/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/tweetybert/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Documents/projects/tweety_bert_paper/src/data_class.py:19\u001b[0m, in \u001b[0;36mSongDataSet_Image.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     17\u001b[0m file_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_paths[index]\n\u001b[1;32m     18\u001b[0m data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(file_path, allow_pickle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 19\u001b[0m spectogram \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39ms\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     20\u001b[0m \u001b[39m# spectogram = spectogram[20:216]\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \u001b[39m# Remove when free from Yarden's Spec Gen Method \u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m# Z-score normalization\u001b[39;00m\n\u001b[1;32m     24\u001b[0m mean \u001b[39m=\u001b[39m spectogram\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/anaconda3/envs/tweetybert/lib/python3.11/site-packages/numpy/lib/npyio.py:253\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39mif\u001b[39;00m magic \u001b[39m==\u001b[39m \u001b[39mformat\u001b[39m\u001b[39m.\u001b[39mMAGIC_PREFIX:\n\u001b[1;32m    252\u001b[0m     \u001b[39mbytes\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mzip\u001b[39m.\u001b[39mopen(key)\n\u001b[0;32m--> 253\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mformat\u001b[39m\u001b[39m.\u001b[39mread_array(\u001b[39mbytes\u001b[39m,\n\u001b[1;32m    254\u001b[0m                              allow_pickle\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mallow_pickle,\n\u001b[1;32m    255\u001b[0m                              pickle_kwargs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpickle_kwargs,\n\u001b[1;32m    256\u001b[0m                              max_header_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_header_size)\n\u001b[1;32m    257\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mzip\u001b[39m.\u001b[39mread(key)\n",
      "File \u001b[0;32m~/anaconda3/envs/tweetybert/lib/python3.11/site-packages/numpy/lib/format.py:776\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    774\u001b[0m version \u001b[39m=\u001b[39m read_magic(fp)\n\u001b[1;32m    775\u001b[0m _check_version(version)\n\u001b[0;32m--> 776\u001b[0m shape, fortran_order, dtype \u001b[39m=\u001b[39m _read_array_header(\n\u001b[1;32m    777\u001b[0m         fp, version, max_header_size\u001b[39m=\u001b[39mmax_header_size)\n\u001b[1;32m    778\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(shape) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    779\u001b[0m     count \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/tweetybert/lib/python3.11/site-packages/numpy/lib/format.py:629\u001b[0m, in \u001b[0;36m_read_array_header\u001b[0;34m(fp, version, max_header_size)\u001b[0m\n\u001b[1;32m    627\u001b[0m     header \u001b[39m=\u001b[39m _filter_header(header)\n\u001b[1;32m    628\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 629\u001b[0m     d \u001b[39m=\u001b[39m safe_eval(header)\n\u001b[1;32m    630\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mSyntaxError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    631\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCannot parse header: \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/tweetybert/lib/python3.11/site-packages/numpy/lib/utils.py:1083\u001b[0m, in \u001b[0;36msafe_eval\u001b[0;34m(source)\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[39m# Local import to speed up numpy's import time.\u001b[39;00m\n\u001b[1;32m   1082\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mast\u001b[39;00m\n\u001b[0;32m-> 1083\u001b[0m \u001b[39mreturn\u001b[39;00m ast\u001b[39m.\u001b[39mliteral_eval(source)\n",
      "File \u001b[0;32m~/anaconda3/envs/tweetybert/lib/python3.11/ast.py:64\u001b[0m, in \u001b[0;36mliteral_eval\u001b[0;34m(node_or_string)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39mEvaluate an expression node or a string containing only a Python\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39mexpression.  The string or node provided may only consist of the following\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mCaution: A complex expression can overflow the C stack and cause a crash.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(node_or_string, \u001b[39mstr\u001b[39m):\n\u001b[0;32m---> 64\u001b[0m     node_or_string \u001b[39m=\u001b[39m parse(node_or_string\u001b[39m.\u001b[39mlstrip(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m), mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39meval\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(node_or_string, Expression):\n\u001b[1;32m     66\u001b[0m     node_or_string \u001b[39m=\u001b[39m node_or_string\u001b[39m.\u001b[39mbody\n",
      "File \u001b[0;32m~/anaconda3/envs/tweetybert/lib/python3.11/ast.py:50\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(source, filename, mode, type_comments, feature_version)\u001b[0m\n\u001b[1;32m     48\u001b[0m     feature_version \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[39m# Else it should be an int giving the minor version for 3.x.\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcompile\u001b[39m(source, filename, mode, flags,\n\u001b[1;32m     51\u001b[0m                _feature_version\u001b[39m=\u001b[39mfeature_version)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = LinearProbeTrainer(model=classifier_model, train_loader=train_loader, test_loader=test_loader, device=device, lr=1e-3, plotting=True, batches_per_eval=100, desired_total_batches=1e4, patience=4)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/142 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 142/142 [00:05<00:00, 27.74batch/s]\n"
     ]
    }
   ],
   "source": [
    "from linear_probe import ModelEvaluator\n",
    "\n",
    "\n",
    "# Initialize the ModelEvaluator with the filter_unseen_classes feature\n",
    "evaluator = ModelEvaluator(model=classifier_model, \n",
    "                           test_loader=test_loader, \n",
    "                           num_classes=21,  # Assuming there are 21 possible classes\n",
    "                           device='cuda:0',  # Use CUDA if available\n",
    "                           use_tqdm=True,  # Enable a progress bar for the evaluation\n",
    "                           filter_unseen_classes=True,  # Enable filtering based on training set classes\n",
    "                           train_dir=train_dir)  # Path to the training dataset directory\n",
    "\n",
    "# Perform model validation with multiple passes\n",
    "class_frame_error_rates, total_frame_error_rate = evaluator.validate_model_multiple_passes(num_passes=1, max_batches=1250)\n",
    "\n",
    "# Save the results to a specified directory, for example 'evaluation_results'\n",
    "evaluator.save_results(class_frame_error_rates, total_frame_error_rate, '')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "canary-vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
