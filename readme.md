# TweetyBERT

## Installation & Usage

### Overview

TweetyBERT combines a convolutional front-end with a transformer architecture to learn representations of bird vocalizations. The model can be used for:

- Self-supervised pretraining on unlabeled bird songs  
- Linear probing for syllable classification  
- Generating embeddings for visualization and analysis  
- Decoding syllable sequences  

### Prerequisites

- Python 3.11+  
- PyTorch >= 2.0  
- CUDA 12.x (for GPU acceleration)  
- Required packages: `numpy`, `matplotlib`, `tqdm`, `umap-learn`, `hdbscan`, `scikit-learn`, `pandas`, `seaborn`, `jupyter`, `ipykernel`, `librosa`, `soundfile`, `shutil-extra`

### Installation Steps

Below is a sample workflow to get you started with Conda, including installing all the necessary dependencies:

# 1. Create and activate a new Conda environment
conda create -n tweetybert python=3.11
conda activate tweetybert

# 2. Install core scientific packages
conda install -c conda-forge \
    numpy \
    matplotlib \
    tqdm \
    umap-learn \
    hdbscan \
    scikit-learn \
    pandas \
    seaborn \
    jupyter \
    ipykernel

# 3. Install audio processing libraries
conda install -c conda-forge \
    librosa \
    soundfile

# 4. Install additional dependencies via pip
pip install shutil-extra

# 5. (Optional) Install PyTorch if not already installed
#    Note: Adjust CUDA version if necessary.
#    For more details, see https://pytorch.org/get-started/locally/
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu12

# 6. Clone the TweetyBERT repository
git clone https://github.com/yourusername/TweetyBERT.git
cd TweetyBERT

A self-supervised transformer-based model for analyzing and decoding bird vocalizations, with a focus on canary song analysis.

## Song Detection & JSON Format

TweetyBERT uses a song detection file (JSON) to locate regions within each recording where bird song is present. A single recording file can contain multiple songs, and each song is stored in a separate list or segment. The JSON also supports **optional** syllable labels for more fine-grained annotation.

Below is a **simplified** example of how a single entry in the song detection JSON might look. Placeholder values are used here:

'''json
{
  "filename": "bird_XXXX_YYYY_MM_DD_HH_MM_SS.wav",
  "song_present": true,
  "segments": [
    {
      "onset_timebin": 100,
      "offset_timebin": 500,
      "onset_ms": 1234.56,
      "offset_ms": 5678.90
    },
    {
      "onset_timebin": 600,
      "offset_timebin": 900,
      "onset_ms": 7890.12,
      "offset_ms": 12345.67
    }
  ],
  "spec_parameters": {
    "step_size": 119,
    "nfft": 1024
  },
  "syllable_labels": {
    "1": [
      [1.00, 2.50],
      [3.75, 4.10]
    ],
    "7": [
      [5.20, 5.45]
    ]
  }
}
'''

- **filename**: The WAV file name.  
- **song_present**: Whether any song was detected in this file.  
- **segments**: Lists each detected song segment using onset and offset information (in both timebins and milliseconds).  
- **spec_parameters**: Parameters (like \`step_size\` and \`nfft\`) used for spectrogram generation.  
- **syllable_labels (optional)**: Time intervals for each labeled syllable, keyed by the label ID.

This JSON is generated by a separate Song Detector tool. You can create your own or adapt an existing Song Detection tool. TweetyBERT simply needs to know where the songs are in each recording and optionally any known syllable label intervals.

## Pretraining

TweetyBERT can be pretrained on any set of WAV files where the bird songs are marked in a corresponding JSON. Below are the only parameters you must edit for the pretraining script:

```bash
INPUT_DIR="/home/george-vengrovski/Documents/testost_pretrain"
SONG_DETECTION_JSON_PATH=None
TEST_PERCENTAGE=20
EXPERIMENT_NAME="TESTOSTERONE_MODEL"
```

- **INPUT_DIR**: Path to the folder containing WAV files.  
- **SONG_DETECTION_JSON_PATH**: Path to the JSON file (or \`None\` if you don't have one).  
- **TEST_PERCENTAGE**: By default, 20% of data is used for testing. You can change this as needed.  
- **EXPERIMENT_NAME**: Name of the training run; the output will be stored in the \`/experiments\` directory under this name.

Nothing else in the pretraining script needs to be changed.

## Training a Decoder

After pretraining, you can train a decoder (linear classifier) to label syllables (or cluster IDs, etc.). You can use either:

- \`train_decoder.sh\`  
- \`train_decoder_multiple_dir.sh\` (which allows selecting specific dates/subfolders)

Below is an example of key variables in a decoder script:

```bash
BIRD_NAME="LLb3_test_with_modification_toscrtipt"
MODEL_NAME="LLB_Model_For_Paper"
WAV_FOLDER="/media/george-vengrovski/George-SSD/llb_stuff/llb_birds/yarden_data/llb3_songs"
SONG_DETECTION_JSON_PATH="/media/george-vengrovski/disk2/canary/yarden_data/llb3_data/onset_offset_results.json"
NUM_SAMPLES=15
```

- **BIRD_NAME**: A short descriptive name for the bird. Used in UMAP visualization and inference outputs.  
- **MODEL_NAME**: The name of the pretrained model from the \`EXPERIMENT_NAME\` set during pretraining.  
- **WAV_FOLDER**: Path to the original WAV files for this bird.  
- **SONG_DETECTION_JSON_PATH**: The path to the detection JSON used for these files.  
- **NUM_SAMPLES**: (Optional) If you only want to generate embeddings or train the decoder on a subset of files.

## Inference

To run inference on new WAV files and produce a decoded database:

```bash
WAV_FOLDER="/media/george-vengrovski/disk2/canary/yarden_data/llb3_data/llb3_songs"
SONG_DETECTION_JSON_PATH="/media/george-vengrovski/disk2/canary/yarden_data/llb3_data/onset_offset_results.json"
BIRD_NAME="llb3"
APPLY_POST_PROCESSING="True"
```

- **WAV_FOLDER**: Path to new WAV files.  
- **SONG_DETECTION_JSON_PATH**: JSON path indicating song segments for these files.  
- **BIRD_NAME**: Bird name (should match or be consistent with training).  
- **APPLY_POST_PROCESSING**: If \`"True"\`, TweetyBERT applies additional merging or cleaning of short segments in the final output.

Running inference creates a JSON database capturing file-by-file results, such as whether song was present and the detected syllables (with onset/offset times). This can be used for further labeling or analysis. For example:

```json
{
  "metadata": {
    "classifier_path": "experiments/LLB_Model_For_Paper",
    "spec_dst_folder": "imgs/decoder_specs_inference_test",
    "output_path": "files/llb3_decoded_database.json",
    "visualize": false,
    "dump_interval": 500,
    "apply_post_processing": true
  },
  "results": [
    {
      "file_name": "llb3_1688_2018_04_27_12_27_27.wav",
      "creation_date": "2018-05-07T09:08:30",
      "song_present": false,
      "syllable_onsets_offsets_ms": {},
      "syllable_onsets_offsets_timebins": {}
    },
    {
      "file_name": "llb3_3251_2018_05_01_05_48_59.wav",
      "creation_date": "2018-05-07T09:09:59",
      "song_present": true,
      "syllable_onsets_offsets_ms": {
        "1": [
          [0.0, 2579.6825396825398]
        ],
        "3": [
          [2579.6825396825398, 2663.3333333333335],
          [3834.444444444445, 6508.571428571429],
          ...
        ],
        "0": [...],
        "2": [...]
      },
      "syllable_onsets_offsets_timebins": {
        "1": [[0.0, 956]],
        "3": [[956.0, 987], [1421.0, 2412], ...],
        ...
      }
    }
  ]
}
```

The \`metadata\` block describes the inference setup, while each entry in \`results\` provides file-level decoding details.

### Happy TweetyBERTing!

If you have any questions or suggestions, feel free to open an issue or pull request on our TweetyBERT repository.
